{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-aRiOgl4nHg"
      },
      "source": [
        "------\n",
        "**You cannot save any changes you make to this file, so please make sure to save it on your Google Colab drive or download it as a .ipynb file.**\n",
        "\n",
        "------\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIZrAUx57vsM"
      },
      "source": [
        "Practical 1: Sentiment Detection in Movie Reviews\n",
        "========================================\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4kXPMhyngZW"
      },
      "source": [
        "This practical concerns detecting sentiment in movie reviews. This is a typical NLP classification task.\n",
        "In [this file](https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json) (80MB) you will find 1000 positive and 1000 negative **movie reviews**.\n",
        "Each review is a **document** and consists of one or more sentences.\n",
        "\n",
        "To prepare yourself for this practical, you should\n",
        "have a look at a few of these texts to understand the difficulties of\n",
        "the task: how might one go about classifying the texts? You will write\n",
        "code that decides whether a movie review conveys positive or\n",
        "negative sentiment.\n",
        "\n",
        "Please make sure you have read the following paper:\n",
        "\n",
        ">   Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan\n",
        "(2002). \n",
        "[Thumbs up? Sentiment Classification using Machine Learning\n",
        "Techniques](https://dl.acm.org/citation.cfm?id=1118704). EMNLP.\n",
        "\n",
        "Bo Pang et al. introduced the movie review sentiment\n",
        "classification task, and the above paper was one of the first papers on\n",
        "the topic. The first version of your sentiment classifier will do\n",
        "something similar to Pang et al.'s system. If you have questions about it,\n",
        "you should resolve you doubts as soon as possible with your TA.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb7errgRASzZ"
      },
      "source": [
        "**Advice**\n",
        "\n",
        "Please read through the entire practical and familiarise\n",
        "yourself with all requirements before you start coding or otherwise\n",
        "solving the tasks. Writing clean and concise code can make the difference\n",
        "between solving the assignment in a matter of hours, and taking days to\n",
        "run all experiments.\n",
        "\n",
        "## Environment\n",
        "\n",
        "All code should be written in **Python 3**. \n",
        "This is the default in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SaZnxptMJiD7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.6\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYZyIF7lJnGn"
      },
      "source": [
        "If you want to run code on your own computer, then download this notebook through `File -> Download .ipynb`.\n",
        "The easiest way to\n",
        "install Python is through downloading\n",
        "[Anaconda](https://www.anaconda.com/download). \n",
        "After installation, you can start the notebook by typing `jupyter notebook filename.ipynb`.\n",
        "You can also use an IDE\n",
        "such as [PyCharm](https://www.jetbrains.com/pycharm/download/) to make\n",
        "coding and debugging easier. It is good practice to create a [virtual\n",
        "environment](https://docs.python.org/3/tutorial/venv.html) for this\n",
        "project, so that any Python packages don’t interfere with other\n",
        "projects. \n",
        " \n",
        "\n",
        "**Learning Python 3**\n",
        "\n",
        "If you are new to Python 3, you may want to check out a few of these resources:\n",
        "- https://learnxinyminutes.com/docs/python3/\n",
        "- https://www.learnpython.org/\n",
        "- https://docs.python.org/3/tutorial/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "hok-BFu9lGoK"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import sys\n",
        "from subprocess import call\n",
        "from nltk import FreqDist\n",
        "from nltk.util import ngrams\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import sklearn as sk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "import pickle, json\n",
        "from collections import Counter, defaultdict\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXWyGHwE-ieQ"
      },
      "source": [
        "## Loading the data\n",
        "\n",
        "**Download the sentiment lexicon and the movie reviews dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lm-rakqtlMOT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-11-12 22:38:31--  https://gist.githubusercontent.com/bastings/d6f99dcb6c82231b94b013031356ba05/raw/f80a0281eba8621b122012c89c8b5e2200b39fd6/sent_lexicon\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 662577 (647K) [text/plain]\n",
            "Saving to: ‘sent_lexicon.2’\n",
            "\n",
            "sent_lexicon.2      100%[===================>] 647.05K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2022-11-12 22:38:31 (9.33 MB/s) - ‘sent_lexicon.2’ saved [662577/662577]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download sentiment lexicon\n",
        "if not os.path.exists('sent_lexicon'):\n",
        "    !wget https://gist.githubusercontent.com/bastings/d6f99dcb6c82231b94b013031356ba05/raw/f80a0281eba8621b122012c89c8b5e2200b39fd6/sent_lexicon\n",
        "# download review data\n",
        "if not os.path.exists('reviews.json'):\n",
        "    !wget https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkPwuHp5LSuQ"
      },
      "source": [
        "**Load the movie reviews.**\n",
        "\n",
        "Each token in a review comes with its part-of-speech tag. For documentation on POS-tags, see https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "careEKj-mRpl"
      },
      "outputs": [],
      "source": [
        "# file structure:\n",
        "# [\n",
        "#  {\"cv\": integer, \"sentiment\": str, \"content\": list} \n",
        "#  {\"cv\": integer, \"sentiment\": str, \"content\": list} \n",
        "#   ..\n",
        "# ]\n",
        "# where `content` is a list of sentences, \n",
        "# with a sentence being a list of (token, pos_tag) pairs.\n",
        "\n",
        "\n",
        "with open(\"reviews.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  reviews = json.load(f)\n",
        "  \n",
        "print(\"Total number of reviews:\", len(reviews), '\\n')\n",
        "\n",
        "def print_sentence_with_pos(s):\n",
        "  print(\" \".join(\"%s/%s\" % (token, pos_tag) for token, pos_tag in s))\n",
        "\n",
        "for i, r in enumerate(reviews):\n",
        "  print(r[\"cv\"], r[\"sentiment\"], len(r[\"content\"]))  # cv, sentiment, num sents\n",
        "  print_sentence_with_pos(r[\"content\"][0])\n",
        "  if i == 4: \n",
        "    break\n",
        "    \n",
        "c = Counter()\n",
        "for review in reviews:\n",
        "  for sentence in review[\"content\"]:\n",
        "    for token, pos_tag in sentence:\n",
        "      c[token.lower()] += 1\n",
        "      \n",
        "print(\"\\nNumber of word types:\", len(c))\n",
        "print(\"Number of word tokens:\", sum(c.values()))\n",
        "\n",
        "print(\"\\nMost common tokens:\")\n",
        "for token, count in c.most_common(20):\n",
        "  print(\"%10s : %8d\" % (token, count))\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6PWaEoh8B34"
      },
      "source": [
        "#(1) Lexicon-based approach (3.5pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsTSMb6ma4E8"
      },
      "source": [
        "A traditional approach to classify documents according to their sentiment is the lexicon-based approach. To implement this approach, you need a **sentiment lexicon**, i.e., a list of words annotated with a sentiment label (e.g., positive and negative, or a score from 0 to 5).\n",
        "\n",
        "In this practical, you will use the sentiment\n",
        "lexicon released by Wilson et al. (2005).\n",
        "\n",
        "> Theresa Wilson, Janyce Wiebe, and Paul Hoffmann\n",
        "(2005). [Recognizing Contextual Polarity in Phrase-Level Sentiment\n",
        "Analysis](http://www.aclweb.org/anthology/H/H05/H05-1044.pdf). HLT-EMNLP.\n",
        "\n",
        "Pay attention to all the information available in the sentiment lexicon. The field *word1* contains the lemma, *priorpolarity* contains the sentiment label (positive, negative, both, or neutral), *type* gives you the magnitude of the word's sentiment (strong or weak), and *pos1* gives you the part-of-speech tag of the lemma. Some lemmas can have multiple part-of-speech tags and thus multiple entries in the lexicon. The path of the lexicon file is `\"sent_lexicon\"`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ogq0Eq2hQglh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type=weaksubj len=1 word1=abandoned pos1=adj stemmed1=n priorpolarity=negative\n",
            "type=weaksubj len=1 word1=abandonment pos1=noun stemmed1=n priorpolarity=negative\n",
            "type=weaksubj len=1 word1=abandon pos1=verb stemmed1=y priorpolarity=negative\n",
            "type=strongsubj len=1 word1=abase pos1=verb stemmed1=y priorpolarity=negative\n",
            "type=strongsubj len=1 word1=abasement pos1=anypos stemmed1=y priorpolarity=negative\n"
          ]
        }
      ],
      "source": [
        "with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  line_cnt = 0\n",
        "  for line in f:\n",
        "    print(line.strip())\n",
        "    line_cnt += 1\n",
        "    if line_cnt > 4:\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mml4nOtIUBhn"
      },
      "source": [
        "Lexica such as this can be used to solve\n",
        "the classification task without using Machine Learning. For example, one might look up every word $w_1 ... w_n$ in a document, and compute a **binary score**\n",
        "$S_{binary}$ by counting how many words have a positive or a\n",
        "negative label in the sentiment lexicon $SLex$.\n",
        "\n",
        "$$S_{binary}(w_1 w_2 ... w_n) = \\sum_{i = 1}^{n}\\text{sign}(SLex\\big[w_i\\big])$$\n",
        "\n",
        "where $\\text{sign}(SLex\\big[w_i\\big])$ refers to the polarity of $w_i$.\n",
        "\n",
        "**Threshold.** On average, there are more positive than negative words per review (~7.13 more positive than negative per review) to take this bias into account you should use a threshold of **8** (roughly the bias itself) to make it harder to classify as positive.\n",
        "\n",
        "$$\n",
        "\\text{classify}(S_{binary}(w_1 w_2 ... w_n)) = \\bigg\\{\\begin{array}{ll}\n",
        "        \\text{positive} & \\text{if } S_{binary}(w_1w_2...w_n) > threshold\\\\\n",
        "        \\text{negative} & \\text{otherwise}\n",
        "        \\end{array}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOFnMvbeeZrc"
      },
      "source": [
        "#### (Q1.1) Implement this approach and report its classification accuracy. (1 pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 473,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_sent_lexicon(line):\n",
        "    \"\"\"\n",
        "    Parse a line from the sentiment lexicon to dict.\n",
        "    Input:\n",
        "        line: line from the sentiment lexicon\n",
        "    Output:\n",
        "        word, pos: word and part-of-speech tag\n",
        "        lexicon: dict with type, pos, stemming, polarity and length\n",
        "    \"\"\"\n",
        "    line = line.strip().split()\n",
        "    pos_dict = {'adj':'JJ', 'noun':'NN', 'verb':'VB', 'anypos':'ANY', 'adverb':'RB'}\n",
        "    \n",
        "    word = line[2][6:]\n",
        "    type = line[0][5:]\n",
        "    length = line[1][4:]\n",
        "    pos = line[3][5:] \n",
        "    pos = pos_dict[pos] # convert pos tags to nltk pos tags\n",
        "    stemmed = line[4][9:]\n",
        "    polarity = line[5][14:]\n",
        "    \n",
        "    lexicon = {'type': type, 'length': length, 'pos': pos, 'stemmed': stemmed,  'polarity': polarity}\n",
        "\n",
        "    return word, lexicon\n",
        "\n",
        "# create sentiment lexicon\n",
        "sent_lexicon = {}\n",
        "with open('sent_lexicon', mode='r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        word, lexicon = parse_sent_lexicon(line)\n",
        "        sent_lexicon[word] = lexicon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 479,
      "metadata": {
        "id": "ED2aTEYutW1-"
      },
      "outputs": [],
      "source": [
        "def sentiment_classifier(doc, sent_lexicon, magnitude_multiplier=1., threshold=8.0, bias_per_word=0.):\n",
        "    '''\n",
        "        Classify a document as positive or negative based on the sentiment lexicon, using a simple binary classifier.\n",
        "        Inputs: \n",
        "            doc - a list of sentences, where a sentence is a list of (token, pos_tag) pairs\n",
        "            sent_lexicon - a dictionary mapping tokens to sentiment scores\n",
        "            magnitude_multiplier - a multiplier for the magnitude of the sentiment score\n",
        "            threshold - the threshold for the sentiment score (float for absolute threshold, 'relative' for relative threshold)\n",
        "            bias_per_word - avg bias per word in the dataset\n",
        "        Output:\n",
        "            sentiment - negative or positive\n",
        "    '''\n",
        "    # get sentiment scores for each word\n",
        "    polarity_score = {'positive': 1, 'negative': -1, 'neutral': 0, 'both': 0}\n",
        "    polarity_weight = {'strongsubj': magnitude_multiplier, 'weaksubj': 1.}\n",
        "    sentiment_score = 0\n",
        "    for sentence in doc:\n",
        "        for word, pos_tag in sentence:\n",
        "            word = word.lower() # convert to lowercase\n",
        "            if word in sent_lexicon.keys():\n",
        "                if sent_lexicon[word]['pos'] == pos_tag or sent_lexicon[word]['pos'] == 'ANY':\n",
        "                    sentiment_score += polarity_score[sent_lexicon[word]['polarity']] * polarity_weight[sent_lexicon[word]['type']]\n",
        "    if threshold=='relative':\n",
        "        threshold = len(doc)*bias_per_word\n",
        "    sentiment = 'POS' if sentiment_score > threshold else 'NEG'\n",
        "    \n",
        "    return sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 477,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"reviews.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  reviews = json.load(f)\n",
        "sent_labels = [r['sentiment'] for r in reviews]\n",
        "    \n",
        "def pred_sentiment(reviews, sent_lexicon, magnitude_multiplier=1., threshold=8.0, bias_per_word=0.):\n",
        "    '''\n",
        "        Predict the sentiment of a list of reviews using the sentiment lexicon.\n",
        "        Inputs:\n",
        "            reviews - a list of reviews, where a review is a list of sentences, where a sentence is a list of (token, pos_tag) pairs\n",
        "            sent_lexicon - a dictionary mapping tokens to sentiment scores\n",
        "            magnitude_multiplier - a multiplier for the magnitude of the sentiment score\n",
        "            threshold - the threshold for the sentiment score (float for absolute threshold, 'relative' for relative threshold)\n",
        "            bias_per_word - avg bias per word in the dataset\n",
        "        Output:\n",
        "            pred_sent - a list of predicted sentiment labels\n",
        "    '''\n",
        "    sent_preds = []\n",
        "    for review in reviews:\n",
        "        sent_preds.append(sentiment_classifier(review['content'], sent_lexicon, magnitude_multiplier, threshold, bias_per_word))\n",
        "    return sent_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 533,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of sentiment classifer (without magnitude): 0.671\n"
          ]
        }
      ],
      "source": [
        "# token_results should be a list of binary indicators; for example [1, 0, 1, ...] \n",
        "# where 1 indicates a correct classification and 0 an incorrect classification.\n",
        "sent_preds = pred_sentiment(reviews, sent_lexicon, magnitude_multiplier=1., threshold=8.0)\n",
        "token_results = (np.array(sent_preds) == np.array(sent_labels)).astype(int)\n",
        "token_accuracy = np.mean(token_results)\n",
        "print(f'Accuracy of sentiment classifer (without magnitude): {token_accuracy:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twox0s_3eS0V"
      },
      "source": [
        "As the sentiment lexicon also has information about the **magnitude** of\n",
        "sentiment (e.g., *“excellent\"* has the same sentiment _polarity_ as *“good\"* but it has a higher magnitude), we can take a more fine-grained approach by adding up all\n",
        "sentiment scores, and deciding the polarity of the movie review using\n",
        "the sign of the weighted score $S_{weighted}$.\n",
        "\n",
        "$$S_{weighted}(w_1w_2...w_n) = \\sum_{i = 1}^{n}SLex\\big[w_i\\big]$$\n",
        "\n",
        "\n",
        "Make sure you define an appropriate threshold for this approach.\n",
        "\n",
        "#### (Q1.2) Now incorporate magnitude information and report the classification accuracy. Don't forget to use the threshold. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 534,
      "metadata": {
        "id": "qG3hUDnPtkhS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of sentiment classifer (with magnitude): 0.693\n"
          ]
        }
      ],
      "source": [
        "# Sentiment classifier with magnitude\n",
        "sent_preds_magnitude = pred_sentiment(reviews, sent_lexicon, magnitude_multiplier=2., threshold=8.0)\n",
        "magnitude_results = (np.array(sent_preds_magnitude) == np.array(sent_labels)).astype(int)\n",
        "magnitude_accuracy = np.mean(magnitude_results)\n",
        "print(f'Accuracy of sentiment classifer (with magnitude): {magnitude_accuracy:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9SHoGPfsAHV"
      },
      "source": [
        "#### (Q.1.3) Make a barplot of the two results (0.5pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 535,
      "metadata": {
        "id": "8LgBcYcXsEk3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAHFCAYAAAD7ZFORAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/av/WaAAAACXBIWXMAAA9hAAAPYQGoP6dpAABONUlEQVR4nO3deXwM9+M/8Nfm3BwSkUgi5HRGRZGQJkTEEQRF3VpxVtOqmypKJdWmpS2tVlQR1U8QilSJI24l6krQUnXHEXfkEpHj/fvDb+drbRLZ2NhkvJ6Pxz4e2fe+Z+Y9s7PvfWXmPbMKIYQAERERkQwZ6LsBREREROWFQYeIiIhki0GHiIiIZItBh4iIiGSLQYeIiIhki0GHiIiIZItBh4iIiGSLQYeIiIhki0GHiIiIZOuVCzp//fUXevbsCRcXF5iamsLBwQF+fn6YOHFiuS734cOHmDVrFvbs2aPx2vLly6FQKHD58uVybcOLWrlyJebPn6/TeQ4ZMgRubm46nac2Ll++DIVCgeXLl6uVx8bG4rXXXoOZmRkUCgWSk5Mxa9YsKBQK/TSUyuTChQswNTVFYmKiVFbcfqzaF77++uuX2MLilUd7tOlr2rRpgzZt2uhkuefPn8egQYPg4uICMzMz1K5dGxMmTMC9e/d0Mn852LNnDxQKhdp3RHx8PGbNmlUuy3vZ/VlBQQG+/fZbdOrUCbVq1YK5uTk8PT3x8ccf48GDB8+dPi8vD7Vr1y7Td9ArFXQ2b94Mf39/ZGRkYM6cOdi+fTu+++47tGzZErGxseW67IcPHyI8PLzIoNOlSxckJiaiRo0a5dqGF1UeQUffatSogcTERHTp0kUqu3PnDgYNGoTatWtj69atSExMRL169TBixAi1L0yq+CZNmoQOHTrAz89PKpPjflyR3blzB2+88QYOHDiAzz77DPHx8Rg1ahR+/vlntG/fHoWFhfpuYoXQrFkzJCYmolmzZlJZfHw8wsPD9dgq3cnJycGsWbPg6uqK+fPnIz4+Hu+++y4WL16Mli1bIicnp8TpjY2NMXPmTERERGgdkI1epOGVzZw5c+Du7o5t27bByOj/Vr1///6YM2eO3tpVvXp1VK9eXW/Lf5WZmprijTfeUCv777//kJeXh3feeQeBgYFSubm5OWrVqqWzZT98+BDm5uY6m58+VcR1OXPmDOLi4rB169aXsryKuA0qgt9//x337t1DbGws2rVrBwAICgpCbm4upk2bhhMnTqBp06Z6bqX+WVlZafRFcmJmZoZLly7B1tZWKmvTpg1cXFzQp08frFu3Du+8806J8xgwYAAmTJiAn376CdOmTSv1sl+pIzr37t2DnZ2dWshRMTDQ3BSxsbHw8/ODhYUFLC0t0bFjRyQlJanVGTJkCCwtLXH+/HmEhITA0tISzs7OmDhxInJzcwE8OQStCjLh4eFQKBRQKBQYMmQIgKIPJ7dp0waNGjVCYmIi/P39YWZmBjc3N0RHRwN4cnSqWbNmMDc3h5eXV5Gd+blz5zBw4EDY29vD1NQUnp6e+PHHH9XqqA6Xrlq1CtOnT4eTkxOsrKzQvn17nD17Vq09mzdvxpUrV6T2l+aw58qVK+Hn5wdLS0tYWlqiSZMmWLp0aYnT/Pjjj2jdujXs7e1hYWEBLy8vzJkzB3l5eWr1kpKS0LVrV2n9nJyc0KVLF1y7dk2qs3btWvj6+sLa2hrm5ubw8PDAsGHDpNefPXU1ZMgQtGrVCgDQr18/KBQK6fB9cYd6tdlPTp06heDgYFSpUkXq9Ity/vx5DB06FHXr1oW5uTlq1qyJbt264dSpUxp1Hzx4gIkTJ8LDwwOmpqawt7dHSEgI/v33X6lObm4uIiIi4OnpCaVSCVtbWwQFBeHgwYNFboenKRQKtcPnqu1w/Phx9O7dGzY2NqhduzYA4OjRo+jfvz/c3NykfXbAgAG4cuWKxnyvX7+OkSNHwtnZGSYmJnByckLv3r1x69YtZGVloWrVqnjvvfc0prt8+TIMDQ0xd+7cYrcfAERFRcHR0REdOnSQykq7H3/77bdwd3eHpaUl/Pz8cOjQIbXXS3o/Hz9+jNmzZ6NBgwYwNTVF9erVMXToUNy5c0dtHrt27UKbNm1ga2sLMzMzuLi4oFevXnj48KHW7QGAjRs3ws/PD+bm5qhSpQo6dOhQqiOQQgjMmTMHrq6uUCqVaNasGbZs2fLc6UrL2NgYAGBtba1WXrVqVQCAUqnU2bLc3NzQtWtXbNq0CU2bNoWZmRk8PT2xadMmAE/6Wk9PT1hYWKBFixY4evSo2vTa7L9//vkn/Pz8oFQqUbNmTcyYMQNLlizR6MtVbdq6dSuaNWsGMzMzNGjQAMuWLVOb37OnroYMGSL110/vq5cvX9bq8wo8+b5o0qQJTE1N4e7uXuzpUCEEFi5ciCZNmsDMzAw2Njbo3bs3Ll68WNJmLxVDQ0O1kKPSokULAMDVq1efOw8TExP069cPixcvhla/Ry5eISNGjBAAxOjRo8WhQ4fE48ePi637+eefC4VCIYYNGyY2bdok1q9fL/z8/ISFhYX4559/pHqDBw8WJiYmwtPTU3z99ddix44dYubMmUKhUIjw8HAhhBCPHj0SW7duFQDE8OHDRWJiokhMTBTnz58XQggRHR0tAIhLly5J8w0MDBS2traifv36YunSpWLbtm2ia9euAoAIDw8XXl5eYtWqVSI+Pl688cYbwtTUVFy/fl2a/p9//hHW1tbCy8tLrFixQmzfvl1MnDhRGBgYiFmzZkn1du/eLQAINzc38fbbb4vNmzeLVatWCRcXF1G3bl2Rn58vza9ly5bC0dFRan9iYmKJ23vGjBkCgHjrrbfE2rVrxfbt28W3334rZsyYobb9XF1d1aYbP368iIqKElu3bhW7du0S8+bNE3Z2dmLo0KFSnaysLGFrayt8fHzEmjVrxN69e0VsbKwICwsTp0+fFkIIcfDgQaFQKET//v1FfHy82LVrl4iOjhaDBg2S5nPp0iUBQERHRwshhDh//rz48ccfBQDxxRdfiMTEROn9/vTTT8WzHxlt9hNjY2Ph5uYmIiMjxc6dO8W2bduK3XZ79+4VEydOFL/99pvYu3ev2LBhg+jRo4cwMzMT//77r1QvIyNDvPbaa8LCwkJERESIbdu2iXXr1omxY8eKXbt2CSGEyMvLE0FBQcLIyEhMmjRJxMfHi40bN4pp06aJVatWFbkdngZAfPrpp9Jz1XZwdXUVU6ZMEQkJCSIuLk4IIcTatWvFzJkzxYYNG8TevXvF6tWrRWBgoKhevbq4c+eONI9r166JGjVqCDs7O/Htt9+KHTt2iNjYWDFs2DBx5swZaT+wsLAQDx48UGvP5MmThVKpFHfv3i12+wkhhIeHh+jbt69aWUn7sWobuLm5iU6dOom4uDgRFxcnvLy8hI2NjVo7ins/CwoKRKdOnYSFhYUIDw8XCQkJYsmSJaJmzZqiYcOG4uHDh9KylEql6NChg4iLixN79uwRMTExYtCgQSItLU3r9sTExAgAIjg4WMTFxYnY2Fjh7e0tTExMxP79+6V6RfU1qvdz+PDhYsuWLWLx4sWiZs2awtHRUQQGBqptv7y8vFI9CgsLpWkePHggXFxcROvWrcXff/8tMjMzxd69e4WLi4vo1q1bie+htlxdXUWtWrVEo0aNpP7R19dXGBsbi5kzZ4qWLVuK9evXiw0bNoh69eoJBwcH6T0RovT774kTJ4RSqRSNGzcWq1evFhs3bhQhISHCzc1NY/uq2tSwYUOxYsUKsW3bNtGnTx8BQOzdu1eqp+qLd+/eLYR40hf17t1bAFDbVx89eqTV53XHjh3C0NBQtGrVSqxfv16sXbtWNG/eXLi4uGj0Z++++64wNjYWEydOFFu3bhUrV64UDRo0EA4ODuLmzZtSvYKCglLtB6rvj5Ko9snff//9uXWFECI2NlYAECdPnixVfSGEeKWCzt27d0WrVq0EAAFAGBsbC39/fxEZGSkyMzOleikpKcLIyEiMHj1abfrMzEzh6Oio1nkOHjxYABBr1qxRqxsSEiLq168vPb9z547GDqhSXNABII4ePSqV3bt3TxgaGgozMzO1UJOcnCwAiO+//14q69ixo6hVq5ZIT09XW9aHH34olEqluH//vhDi/z5cISEhavXWrFkjfcBUunTpohFKinPx4kVhaGgo3n777RLrFRV0nqb6QK1YsUIYGhpK7T569KgAIH3BFuXrr78WADS+KJ9WVIeh2iZr165Vq/ts0CnLfrJs2bJi21KS/Px88fjxY1G3bl0xfvx4qTwiIkIAEAkJCcVOu2LFCgFA/Pzzz8XWKUvQmTlzZqnanZWVJSwsLMR3330nlQ8bNkwYGxtLobQoFy5cEAYGBmLevHlSWU5OjrC1tVULvUW5deuWACC+/PJLjdeK249V28DLy0utgz58+LAAIIVCIYp/P1etWiUAiHXr1qmVHzlyRAAQCxcuFEII8dtvvwkAIjk5udh1KG17CgoKhJOTk/Dy8hIFBQVSvczMTGFvby/8/f2lsmf7mrS0NKFUKkXPnj3Vln3gwAEBQCPoqPrO5z2e3Y9u3Lgh/Pz81Or06dNHPHr0qNj1LwtXV1dhZmYmrl27JpWp+scaNWqI7OxsqTwuLk4AEBs3bix2fsXtv3369BEWFhZq4aegoEA0bNiwyKCjVCrFlStXpLKcnBxRrVo18d5770llzwYdIYQYNWqURhgRQrvPq6+vr3BychI5OTlSWUZGhqhWrZravBMTEwUA8c0336jN7+rVq8LMzEx89NFHUpmqD3je43nfF9euXRMODg7Cx8dHbd8tyblz5wQAERUVVar6QgjxSp26srW1xf79+3HkyBF8+eWX6N69O/777z9MnToVXl5euHv3LgBg27ZtyM/PR2hoKPLz86WHUqlEYGCgxoBihUKBbt26qZU1bty4yMOd2qhRowa8vb2l59WqVYO9vT2aNGkCJycnqdzT0xMApOU9evQIO3fuRM+ePWFubq62DiEhIXj06JHGoe8333xTo/1Pz1NbCQkJKCgowKhRo7SeNikpCW+++SZsbW1haGgIY2NjhIaGoqCgAP/99x8AoE6dOrCxscGUKVOwaNEinD59WmM+zZs3BwD07dsXa9aswfXr18u0LsXRdj8BgF69epVq3vn5+fjiiy/QsGFDmJiYwMjICCYmJjh37hzOnDkj1duyZQvq1auH9u3bFzuvLVu2QKlUqp2y04Wi1iUrKwtTpkxBnTp1YGRkBCMjI1haWiI7O1uj3UFBQdK+WxQPDw907doVCxculA5Tr1y5Evfu3cOHH35YYttu3LgBALC3t9d6vbp06QJDQ0PpeUmfhWe3waZNm1C1alV069ZNbZ9o0qQJHB0dpX2iSZMmMDExwciRI/HLL7+UeGrgee05e/Ysbty4gUGDBqmdgre0tESvXr1w6NChIk+HAUBiYiIePXqEt99+W63c398frq6uGvWPHDlSqsfT/WFaWhq6d++OjIwMxMTEYN++fVi4cCH+/PNPvPnmm8jPzy923cuiSZMmqFmzpvRctY+1adNGbQzVs/0mUPr9d+/evWjbti3s7OykMgMDA/Tt27fYNrm4uEjPlUol6tWr98LfEc+TnZ2NI0eO4K233lI7RVilShWN76xNmzZBoVDgnXfeUdt3HR0d8frrr6v1ZyNHjizVfvDHH38U27b79+8jJCQEQgjExsYWOXykKKrPtDb9+Ss1GFnFx8cHPj4+AJ5csjZlyhTMmzcPc+bMwZw5c3Dr1i0A//dF+axn3xBzc3ON88ympqZ49OjRC7WzWrVqGmUmJiYa5SYmJgAgLe/evXvIz8/HggULsGDBgiLnrQp1Ks+eOzU1NQWA546EL45qPIK2g3dTUlIQEBCA+vXr47vvvoObmxuUSiUOHz6MUaNGSe2xtrbG3r178fnnn2PatGlIS0tDjRo18O677+KTTz6BsbExWrdujbi4OHz//fcIDQ1Fbm4uXnvtNUyfPh0DBgwo03o9rSz7iZWVVanmPWHCBPz444+YMmUKAgMDYWNjAwMDA4wYMULtPblz545aB1qUO3fuwMnJqdQdSWkVdZXgwIEDsXPnTsyYMQPNmzeHlZUVFAoFQkJCNNpdmn1j7NixaNeuHRISEhAcHIwff/wRfn5+alemFEW1rLKM/yjtZ6Go9/PWrVt48OCB9Jl8lupzV7t2bezYsQNz5szBqFGjkJ2dDQ8PD4wZMwZjx47Vqj2qK1CKej+cnJxQWFiItLS0IgdKq6Z1dHTUeK2osiZNmhS5Xs96Oph99dVXSE5OxpUrV6Q2BgQEoEGDBmjbti1iYmIwePDgUs23NIrrH5/XbwKl33/v3bsHBwcHjWUXVQZovofAk/exrP1raaWlpaGwsLBU7++tW7cghCh2HTw8PNSmLc0/EcWN40xLS0OHDh1w/fp17Nq1S23ez6P6TGuz7V7JoPM0Y2NjfPrpp5g3bx7+/vtvAJBS+m+//VbkfzUVnY2NDQwNDTFo0KBij6i4u7uXaxtUg6+vXbsGZ2fnUk8XFxeH7OxsrF+/Xm3bJycna9T18vLC6tWrIYTAyZMnsXz5ckRERMDMzAwff/wxAKB79+7o3r07cnNzcejQIURGRmLgwIFwc3NTu+S4LLTdT7S5Z8X//vc/hIaG4osvvlArv3v3rjSIE3iynZ8efF2U6tWr488//0RhYWGxYUfVeagG0KuUdBnns+uTnp6OTZs24dNPP5W2v2qe9+/f12jT89oNAG3btkWjRo3www8/wNLSEsePH8f//ve/506nem+eXa4uFfV+2tnZwdbWttgrvapUqSL9HRAQgICAABQUFODo0aNYsGABxo0bBwcHB/Tv37/U7VB9iaampmq8duPGDRgYGMDGxqbEaW/evKnx2s2bNzXucaUaWPw80dHR0sUWycnJqFmzpkYQU/2DoOp39U2b/dfW1lb6R+dpRW3H8lDaz6uNjQ0UCkWx7+/T7OzsoFAosH//filMP+3psoiIiFJd9u7q6qpxz6a0tDS0b98ely5dws6dO6UjlKWlei+ePpr2PK9U0ElNTS3yvx7VIUnV6aCOHTvCyMgIFy5cKPWphud50SMk2jA3N0dQUBCSkpLQuHHjYv+71JY2/4EEBwfD0NAQUVFRWgUK1ZfH0x8qIQR+/vnnEqd5/fXXMW/ePCxfvhzHjx8vsu2BgYGoWrUqtm3bhqSkpBcOOuWxn6goFAqNzmbz5s24fv066tSpI5V17twZM2fOxK5du9C2bdsi59W5c2esWrUKy5cvL/b0lYODA5RKJU6ePKlW/vvvv2vVZiGERruXLFmCgoICjTb9+uuvOHv2LOrXr1/ifMeMGYOwsDCkp6fDwcEBffr0eW5bXF1dYWZmhgsXLmi8Vp7/SXft2hWrV69GQUEBfH19SzWNoaEhfH190aBBA8TExOD48eNaBZ369eujZs2aWLlyJSZNmiR9hrKzs7Fu3TrpSqyivPHGG1AqlYiJiVHbhw8ePIgrV65oBJ0jR46Uqk1P/yPl5OSEnTt34vr162qnlFRXhOnylg0vQpv9NzAwEPHx8bh79670hVtYWIi1a9fqtE1Pf2+YmZlJ5aX9vKquLlu/fj3mzp0rBaTMzEyN00pdu3bFl19+ievXrxd7Ck5l5MiR6Nq1a6nbr6IKORcvXkRCQkKZbiugOs3bsGHDUk/zSgWdjh07olatWujWrRsaNGiAwsJCJCcn45tvvoGlpaV0yNjNzQ0RERGYPn06Ll68iE6dOsHGxga3bt3C4cOHYWFhofVNnKpUqQJXV1f8/vvvaNeuHapVqwY7O7tyuyvwd999h1atWiEgIADvv/8+3NzckJmZifPnz+OPP/7Arl27tJ6nl5cX1q9fj6ioKHh7e8PAwEA6BfgsNzc3TJs2DZ999hlycnIwYMAAWFtb4/Tp07h7926x269Dhw4wMTHBgAED8NFHH+HRo0eIiopCWlqaWr1NmzZh4cKF6NGjBzw8PCCEwPr16/HgwQPpcuKZM2fi2rVraNeuHWrVqoUHDx7gu+++g7Gxsdr9ccqqPPYTla5du2L58uVo0KABGjdujGPHjmHu3LkaXwrjxo1DbGwsunfvjo8//hgtWrRATk4O9u7di65duyIoKAgDBgxAdHQ0wsLCcPbsWQQFBaGwsBB//fUXPD090b9/f+nc/LJly1C7dm28/vrrOHz4MFauXFnqNltZWaF169aYO3eutG/v3bsXS5cuVTsKBTz5j3DLli1o3bo1pk2bBi8vLzx48ABbt27FhAkT0KBBA6nuO++8g6lTp2Lfvn345JNPShXcTUxMir0MW5v9WFv9+/dHTEwMQkJCMHbsWLRo0QLGxsa4du0adu/eje7du6Nnz55YtGgRdu3ahS5dusDFxQWPHj2SLjcuabxVUQwMDDBnzhy8/fbb6Nq1K9577z3k5uZi7ty5ePDgAb788stip7WxscGkSZMwe/ZsjBgxAn369MHVq1cxa9asIk93lGU7jRo1CjExMejQoQM+/vhjODs74++//8bs2bPh4OCgNj4oIiICERER2Llzp9pnVKFQFDvuTVe02X+nT5+OP/74A+3atcP06dNhZmaGRYsWITs7G0DRtyspCy8vLwBPTv917twZhoaG0j+vpf28fvbZZ+jUqRM6dOiAiRMnoqCgAF999RUsLCzUjlS1bNkSI0eOxNChQ3H06FG0bt0aFhYWSE1NxZ9//gkvLy+8//77AJ6E16fHiZZGTk6OdOuN+fPnIz8/X+3zWb16dek2FQBgZGSEwMBA7Ny5U20+hw4dgqGhIVq3bl36hZd62LIMxMbGioEDB4q6desKS0tLYWxsLFxcXMSgQYOKvPojLi5OBAUFCSsrK2FqaipcXV1F7969xY4dO6Q6gwcPFhYWFhrTFnUp8o4dO0TTpk2FqampACAGDx4shCj+qqvXXntNY76urq6iS5cuGuUAxKhRo9TKLl26JIYNGyZq1qwpjI2NRfXq1YW/v7+YPXu2VKe4K4yKGtV///590bt3b1G1alWhUCiKvBrgWStWrBDNmzcXSqVSWFpaiqZNm6rNs6irrv744w/x+uuvC6VSKWrWrCkmT54stmzZonZFwr///isGDBggateuLczMzIS1tbVo0aKFWL58uTSfTZs2ic6dO4uaNWsKExMTYW9vL0JCQtQut32Rq65UXmQ/KU5aWpoYPny4sLe3F+bm5qJVq1Zi//79IjAwUONKmLS0NDF27Fjh4uIijI2Nhb29vejSpYvaZeg5OTli5syZom7dusLExETY2tqKtm3bioMHD0p10tPTxYgRI4SDg4OwsLAQ3bp1E5cvXy72qqunrzhRuXbtmujVq5ewsbERVapUEZ06dRJ///23cHV1lfZ3latXr4phw4YJR0dHYWxsLJycnETfvn3FrVu3NOY7ZMgQYWRkpHY1zfMsXbpUGBoaihs3bqiVF7cfq/aFuXPnaszr2W1Q0vuZl5cnvv76a2kftrS0FA0aNBDvvfeeOHfunBDiyRUuPXv2FK6ursLU1FTY2tqKwMBAtSuAtGmPEE/2Q19fX6FUKoWFhYVo166dOHDggFqdovqawsJCERkZKZydnYWJiYlo3Lix+OOPP4rc18rq+PHjomfPnqJWrVrC1NRUeHh4iBEjRoiUlBS1eqp96+krjzIzMwUA0b9//+cuR9v+8dntq83+u3//fuHr6ytMTU2Fo6OjmDx5svjqq680rvQsrk3Pbt+irrrKzc0VI0aMENWrV5f2VdV7V9rPqxBCbNy4UTRu3FiYmJgIFxcX8eWXXxbbny1btkz4+voKCwsLYWZmJmrXri1CQ0PVrgAuC9X2Lu7x7PZFEVf9CSFEQECA1rclUPz/GRIRVUiPHz+Gm5sbWrVqhTVr1pR6ukePHsHFxQUTJ07ElClTyrGFVJ7i4+PRtWtXnDhxQjrCUVEFBwfj8uXL0tWhpFsXLlxA3bp1sW3bNrUbgT7PK3Xqiogqjzt37uDs2bOIjo7GrVu31AaIloZSqUR4eDhmzZqFDz/8EBYWFuXUUipPu3fvRv/+/StcyJkwYQKaNm0KZ2dn3L9/HzExMUhISHjund+p7GbPno127dppFXIABh0iqqA2b96MoUOHokaNGli4cOFzLykvysiRI/HgwQNcvHixwn1RUuk876c+9KWgoAAzZ87EzZs3oVAo0LBhQ/z666/P/b0mKpv8/HzUrl0bU6dO1XpanroiIiIi2dLrnZH37duHbt26wcnJCQqFAnFxcc+dZu/evfD29oZSqYSHhwcWLVpU/g0lIiKiSkmvQSc7Oxuvv/46fvjhh1LVv3TpEkJCQhAQEICkpCRMmzYNY8aMwbp168q5pURERFQZVZhTVwqFAhs2bECPHj2KrTNlyhRs3LhR7TdHwsLCcOLECenmU0REREQqlWowcmJiIoKDg9XKOnbsiKVLlyIvL6/I25Pn5uaq3Sa7sLAQ9+/fh62trVa35CciIiL9EUIgMzNT69/uq1RB5+bNmxo/OObg4ID8/HzcvXu3yJ93iIyMLPPdaYmIiKhiuXr1qlY/HVKpgg6g+UN6qjNvxR2dmTp1KiZMmCA9T09Ph4uLC65evVrqX5ImIiIi/crIyICzs7Paj+OWRqUKOo6Ojhq/uHr79m0YGRlJv8L7LFNT0yJ/idXKyopBh4iIqJLRdtiJXq+60pafnx8SEhLUyrZv3w4fH58ix+cQERHRq02vQScrKwvJyclITk4G8OTy8eTkZKSkpAB4ctopNDRUqh8WFoYrV65gwoQJOHPmDJYtW4alS5di0qRJ+mg+ERERVXB6PXV19OhRBAUFSc9VY2kGDx6M5cuXIzU1VQo9AODu7o74+HiMHz8eP/74I5ycnPD999+jV69eL73tREREVPFVmPvovCwZGRmwtrZGeno6x+gQERFVEmX9/q5UY3SIiIiItMGgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESypfegs3DhQri7u0OpVMLb2xv79+8vsX5MTAxef/11mJubo0aNGhg6dCju3bv3klpLRERElYleg05sbCzGjRuH6dOnIykpCQEBAejcuTNSUlKKrP/nn38iNDQUw4cPxz///IO1a9fiyJEjGDFixEtuOREREVUGeg063377LYYPH44RI0bA09MT8+fPh7OzM6Kiooqsf+jQIbi5uWHMmDFwd3dHq1at8N577+Ho0aMvueVERERUGegt6Dx+/BjHjh1DcHCwWnlwcDAOHjxY5DT+/v64du0a4uPjIYTArVu38Ntvv6FLly7FLic3NxcZGRlqDyIiIno16C3o3L17FwUFBXBwcFArd3BwwM2bN4ucxt/fHzExMejXrx9MTEzg6OiIqlWrYsGCBcUuJzIyEtbW1tLD2dlZp+tBREREFZfeByMrFAq150IIjTKV06dPY8yYMZg5cyaOHTuGrVu34tKlSwgLCyt2/lOnTkV6err0uHr1qk7bT0RERBWXkb4WbGdnB0NDQ42jN7dv39Y4yqMSGRmJli1bYvLkyQCAxo0bw8LCAgEBAZg9ezZq1KihMY2pqSlMTU11vwJERERU4entiI6JiQm8vb2RkJCgVp6QkAB/f/8ip3n48CEMDNSbbGhoCODJkSAiIiKip+n11NWECROwZMkSLFu2DGfOnMH48eORkpIinYqaOnUqQkNDpfrdunXD+vXrERUVhYsXL+LAgQMYM2YMWrRoAScnJ32tBhEREVVQejt1BQD9+vXDvXv3EBERgdTUVDRq1Ajx8fFwdXUFAKSmpqrdU2fIkCHIzMzEDz/8gIkTJ6Jq1apo27YtvvrqK32tAhEREVVgCvGKnfPJyMiAtbU10tPTYWVlpe/mEBERUSmU9ftb71ddEREREZUXBh0iIiKSLQYdIiIiki0GHSIiIpItBh0iIiKSLQYdIiIiki0GHSIiIpItBh0iIiKSLQYdIiIiki0GHSIiIpItBh0iIiKSLQYdIiIiki0GHSIiIpItBh0iIiKSLQYdIiIiki0GHSIiIpItBh0iIiKSLQYdIiIiki0GHSIiIpItBh0iIiKSLQYdIiIiki0GHSIiIpItBh0iIiKSLQYdIiIiki0GHSIiIpItBh0iIiKSLQYdIiIiki0GHSIiIpItBh0iIiKSLQYdIiIiki0GHSIiIpItBh0iIiKSLQYdIiIiki0jfTeAiIj0SKHQdwtIjoTQdwskPKJDREREssWgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESyxaBDREREssX76OgYb0lB5aEC3ZKCiKhS4REdIiIiki0GHSIiIpItBh0iIiKSLQYdIiIiki0GHSIiIpItBh0iIiKSLQYdIiIiki0GHSIiIpItBh0iIiKSLQYdIiIiki0GHSIiIpItBh0iIiKSLQYdIiIiki0GHSIiIpItBh0iIiKSLQYdIiIiki0GHSIiIpItBh0iIiKSLQYdIiIiki0GHSIiIpItBh0iIiKSLb0HnYULF8Ld3R1KpRLe3t7Yv39/ifVzc3Mxffp0uLq6wtTUFLVr18ayZcteUmuJiIioMjHS58JjY2Mxbtw4LFy4EC1btsRPP/2Ezp074/Tp03BxcSlymr59++LWrVtYunQp6tSpg9u3byM/P/8lt5yIiIgqA4UQQuhr4b6+vmjWrBmioqKkMk9PT/To0QORkZEa9bdu3Yr+/fvj4sWLqFatWpmWmZGRAWtra6Snp8PKyqrMbS+OQqHzWRJBf59Skj12WlQeyqHTKuv3t95OXT1+/BjHjh1DcHCwWnlwcDAOHjxY5DQbN26Ej48P5syZg5o1a6JevXqYNGkScnJyil1Obm4uMjIy1B5ERET0atDbqau7d++ioKAADg4OauUODg64efNmkdNcvHgRf/75J5RKJTZs2IC7d+/igw8+wP3794sdpxMZGYnw8HCdt5+IiIgqPr0PRlY8c9hUCKFRplJYWAiFQoGYmBi0aNECISEh+Pbbb7F8+fJij+pMnToV6enp0uPq1as6XwciIiKqmPR2RMfOzg6GhoYaR29u376tcZRHpUaNGqhZsyasra2lMk9PTwghcO3aNdStW1djGlNTU5iamuq28URERFQp6O2IjomJCby9vZGQkKBWnpCQAH9//yKnadmyJW7cuIGsrCyp7L///oOBgQFq1apVru0lIiKiykevp64mTJiAJUuWYNmyZThz5gzGjx+PlJQUhIWFAXhy2ik0NFSqP3DgQNja2mLo0KE4ffo09u3bh8mTJ2PYsGEwMzPT12oQERFRBaXX++j069cP9+7dQ0REBFJTU9GoUSPEx8fD1dUVAJCamoqUlBSpvqWlJRISEjB69Gj4+PjA1tYWffv2xezZs/W1CkRERFSB6fU+OvrA++hQZfRqfUrppWKnReWB99EhIiIiKn9aBx03NzdERESonVIiIiIiqoi0DjoTJ07E77//Dg8PD3To0AGrV69Gbm5uebSNiIiI6IVoHXRGjx6NY8eO4dixY2jYsCHGjBmDGjVq4MMPP8Tx48fLo41EREREZfLCg5Hz8vKwcOFCTJkyBXl5eWjUqBHGjh2LoUOHFnuHY33iYGSqjDgYmcoNOy0qDxVoMHKZLy/Py8vDhg0bEB0djYSEBLzxxhsYPnw4bty4genTp2PHjh1YuXJlWWdPRERE9MK0DjrHjx9HdHQ0Vq1aBUNDQwwaNAjz5s1DgwYNpDrBwcFo3bq1ThtKREREpC2tg07z5s3RoUMHREVFoUePHjA2Ntao07BhQ/Tv318nDSQiIiIqK62DzsWLF6U7FxfHwsIC0dHRZW4UERERkS5ofdXV7du38ddff2mU//XXXzh69KhOGkVERESkC1oHnVGjRuHq1asa5devX8eoUaN00igiIiIiXdA66Jw+fRrNmjXTKG/atClOnz6tk0YRERER6YLWQcfU1BS3bt3SKE9NTYWRkV5/DJ2IiIhIjdZBp0OHDpg6dSrS09OlsgcPHmDatGno0KGDThtHRERE9CK0PgTzzTffoHXr1nB1dUXTpk0BAMnJyXBwcMCvv/6q8wYSERERlZXWQadmzZo4efIkYmJicOLECZiZmWHo0KEYMGBAkffUISIiItKXMg2qsbCwwMiRI3XdFiIiIiKdKvPo4dOnTyMlJQWPHz9WK3/zzTdfuFFEREREulCmOyP37NkTp06dgkKhgOrHz1W/VF5QUKDbFhIRERGVkdZXXY0dOxbu7u64desWzM3N8c8//2Dfvn3w8fHBnj17yqGJRERERGWj9RGdxMRE7Nq1C9WrV4eBgQEMDAzQqlUrREZGYsyYMUhKSiqPdhIRERFpTesjOgUFBbC0tAQA2NnZ4caNGwAAV1dXnD17VretIyIiInoBWh/RadSoEU6ePAkPDw/4+vpizpw5MDExweLFi+Hh4VEebSQiIiIqE62DzieffILs7GwAwOzZs9G1a1cEBATA1tYWsbGxOm8gERERUVkphOqyqRdw//592NjYSFdeVWQZGRmwtrZGeno6rKysdD7/SrAJqBJ68U8pUTHYaVF5KIdOq6zf31qN0cnPz4eRkRH+/vtvtfJq1apVipBDRERErxatgo6RkRFcXV15rxwiIiKqFLS+6uqTTz7B1KlTcf/+/fJoDxEREZHOaD0Y+fvvv8f58+fh5OQEV1dXWFhYqL1+/PhxnTWOiIiI6EVoHXR69OhRDs0gIiIi0j2dXHVVmfCqK6qMXq1PKb1U7LSoPFTWq66IiIiIKhOtT10ZGBiUeCk5r8giIiKiikLroLNhwwa153l5eUhKSsIvv/yC8PBwnTWMiIiI6EXpbIzOypUrERsbi99//10Xsys3HKNDlRHH6FC5YadF5UGOY3R8fX2xY8cOXc2OiIiI6IXpJOjk5ORgwYIFqFWrli5mR0RERKQTWo/RefbHO4UQyMzMhLm5Of73v//ptHFEREREL0LroDNv3jy1oGNgYIDq1avD19cXNjY2Om0cERER0YvQOugMGTKkHJpBREREpHtaj9GJjo7G2rVrNcrXrl2LX375RSeNIiIiItIFrYPOl19+CTs7O41ye3t7fPHFFzppFBEREZEuaB10rly5And3d41yV1dXpKSk6KRRRERERLqgddCxt7fHyZMnNcpPnDgBW1tbnTSKiIiISBe0Djr9+/fHmDFjsHv3bhQUFKCgoAC7du3C2LFj0b9///JoIxEREVGZaH3V1ezZs3HlyhW0a9cORkZPJi8sLERoaCjH6BAREVGFUubfujp37hySk5NhZmYGLy8vuLq66rpt5YK/dUWVEX/risoNOy0qDxXot660PqKjUrduXdStW7eskxMRERGVO63H6PTu3RtffvmlRvncuXPRp08fnTSKiIiISBe0Djp79+5Fly5dNMo7deqEffv26aRRRERERLqgddDJysqCiYmJRrmxsTEyMjJ00igiIiIiXdA66DRq1AixsbEa5atXr0bDhg110igiIiIiXdB6MPKMGTPQq1cvXLhwAW3btgUA7Ny5EytXrsRvv/2m8wYSERERlZXWQefNN99EXFwcvvjiC/z2228wMzPD66+/jl27dpXL5dpEREREZVXm++ioPHjwADExMVi6dClOnDiBgoICXbWtXPA+OlQZ8T46VG7YaVF5qED30dF6jI7Krl278M4778DJyQk//PADQkJCcPTo0bLOjoiIiEjntDp1de3aNSxfvhzLli1DdnY2+vbti7y8PKxbt44DkYmIiKjCKfURnZCQEDRs2BCnT5/GggULcOPGDSxYsKA820ZERET0Qkp9RGf79u0YM2YM3n//ff70AxEREVUKpT6is3//fmRmZsLHxwe+vr744YcfcOfOnfJsGxEREdELKXXQ8fPzw88//4zU1FS89957WL16NWrWrInCwkIkJCQgMzOzPNtJREREpLUXurz87NmzWLp0KX799Vc8ePAAHTp0wMaNG3XZPp3j5eVUGfHycio37LSoPMjh8nIAqF+/PubMmYNr165h1apVLzIrIiIiIp17oaCjYmhoiB49epTpaM7ChQvh7u4OpVIJb29v7N+/v1TTHThwAEZGRmjSpInWyyQiIqJXg06CTlnFxsZi3LhxmD59OpKSkhAQEIDOnTsjJSWlxOnS09MRGhqKdu3avaSWEhERUWX0wj8B8SJ8fX3RrFkzREVFSWWenp7o0aMHIiMji52uf//+qFu3LgwNDREXF4fk5ORSL5NjdKgy4hgdKjfstKg8yGWMzot4/Pgxjh07huDgYLXy4OBgHDx4sNjpoqOjceHCBXz66aelWk5ubi4yMjLUHkRERPRq0FvQuXv3LgoKCuDg4KBW7uDggJs3bxY5zblz5/Dxxx8jJiYGRkalu9dhZGQkrK2tpYezs/MLt52IiIgqB72O0QEAxTOHTYUQGmUAUFBQgIEDByI8PBz16tUr9fynTp2K9PR06XH16tUXbjMRERFVDlr9qKcu2dnZwdDQUOPoze3btzWO8gBAZmYmjh49iqSkJHz44YcAgMLCQgghYGRkhO3bt6Nt27Ya05mamsLU1LR8VoKIiIgqNL0d0TExMYG3tzcSEhLUyhMSEuDv769R38rKCqdOnUJycrL0CAsLQ/369ZGcnAxfX9+X1XQiIiKqJPR2RAcAJkyYgEGDBsHHxwd+fn5YvHgxUlJSEBYWBuDJaafr169jxYoVMDAwQKNGjdSmt7e3h1Kp1CgnIiIiAvQcdPr164d79+4hIiICqampaNSoEeLj4+Hq6goASE1Nfe49dYiIiIiKo9f76OgD76NDldGr9Smll4qdFpUH3keHiIiIqPwx6BAREZFsMegQERGRbDHoEBERkWwx6BAREZFsMegQERGRbDHoEBERkWwx6BAREZFsMegQERGRbDHoEBERkWwx6BAREZFsMegQERGRbDHoEBERkWwx6BAREZFsMegQERGRbDHoEBERkWwx6BAREZFsMegQERGRbDHoEBERkWwx6BAREZFsMegQERGRbDHoEBERkWwx6BAREZFsMegQERGRbDHoEBERkWwx6BAREZFsMegQERGRbDHoEBERkWwx6BAREZFsMegQERGRbDHoEBERkWwx6BAREZFsMegQERGRbDHoEBERkWwx6BAREZFsMegQERGRbDHoEBERkWwx6BAREZFsMegQERGRbDHoEBERkWwx6BAREZFsMegQERGRbDHoEBERkWwx6BAREZFsMegQERGRbDHoEBERkWwx6BAREZFsMegQERGRbDHoEBERkWwx6BAREZFsMegQERGRbDHoEBERkWwx6BAREZFsMegQERGRbDHoEBERkWwx6BAREZFsMegQERGRbDHoEBERkWwx6BAREZFsMegQERGRbDHoEBERkWwx6BAREZFsMegQERGRbOk96CxcuBDu7u5QKpXw9vbG/v37i627fv16dOjQAdWrV4eVlRX8/Pywbdu2l9haIiIiqkz0GnRiY2Mxbtw4TJ8+HUlJSQgICEDnzp2RkpJSZP19+/ahQ4cOiI+Px7FjxxAUFIRu3bohKSnpJbeciIiIKgOFEELoa+G+vr5o1qwZoqKipDJPT0/06NEDkZGRpZrHa6+9hn79+mHmzJmlqp+RkQFra2ukp6fDysqqTO0uiUKh81kSQX+fUpI9dlpUHsqh0yrr97fejug8fvwYx44dQ3BwsFp5cHAwDh48WKp5FBYWIjMzE9WqVSu2Tm5uLjIyMtQeRERE9GrQW9C5e/cuCgoK4ODgoFbu4OCAmzdvlmoe33zzDbKzs9G3b99i60RGRsLa2lp6ODs7v1C7iYiIqPLQ+2BkxTOHTYUQGmVFWbVqFWbNmoXY2FjY29sXW2/q1KlIT0+XHlevXn3hNhMREVHlYKSvBdvZ2cHQ0FDj6M3t27c1jvI8KzY2FsOHD8fatWvRvn37EuuamprC1NT0hdtLRERElY/ejuiYmJjA29sbCQkJauUJCQnw9/cvdrpVq1ZhyJAhWLlyJbp06VLezSQiIqJKTG9HdABgwoQJGDRoEHx8fODn54fFixcjJSUFYWFhAJ6cdrp+/TpWrFgB4EnICQ0NxXfffYc33nhDOhpkZmYGa2trva0HERERVUx6DTr9+vXDvXv3EBERgdTUVDRq1Ajx8fFwdXUFAKSmpqrdU+enn35Cfn4+Ro0ahVGjRknlgwcPxvLly19284mIiKiC0+t9dPSB99GhyujV+pTSS8VOi8oD76NDREREVP4YdIiIiEi2GHSIiIhIthh0iIiISLYYdIiIiEi2GHSIiIhIthh0iIiISLYYdIiIiEi2GHSIiIhIthh0iIiISLYYdIiIiEi2GHSIiIhIthh0iIiISLYYdIiIiEi2GHSIiIhIthh0iIiISLYYdIiIiEi2GHSIiIhIthh0iIiISLYYdIiIiEi2GHSIiIhIthh0iIiISLYYdIiIiEi2GHSIiIhIthh0iIiISLYYdIiIiEi2GHSIiIhIthh0iIiISLYYdIiIiEi2GHSIiIhIthh0iIiISLYYdIiIiEi2GHSIiIhIthh0iIiISLYYdIiIiEi2GHSIiIhIthh0iIiISLYYdIiIiEi2GHSIiIhIthh0iIiISLYYdIiIiEi2GHSIiIhIthh0iIiISLYYdIiIiEi2GHSIiIhIthh0iIiISLYYdIiIiEi2GHSIiIhIthh0iIiISLYYdIiIiEi2GHSIiIhIthh0iIiISLYYdIiIiEi2GHSIiIhIthh0iIiISLYYdIiIiEi2GHSIiIhIthh0iIiISLYYdIiIiEi2GHSIiIhIthh0iIiISLYYdIiIiEi29B50Fi5cCHd3dyiVSnh7e2P//v0l1t+7dy+8vb2hVCrh4eGBRYsWvaSWEhERUWWj16ATGxuLcePGYfr06UhKSkJAQAA6d+6MlJSUIutfunQJISEhCAgIQFJSEqZNm4YxY8Zg3bp1L7nlREREVBkohBBCXwv39fVFs2bNEBUVJZV5enqiR48eiIyM1Kg/ZcoUbNy4EWfOnJHKwsLCcOLECSQmJpZqmRkZGbC2tkZ6ejqsrKxefCWeoVDofJZE0N+nlGSPnRaVh3LotMr6/a23IzqPHz/GsWPHEBwcrFYeHByMgwcPFjlNYmKiRv2OHTvi6NGjyMvLK7e2EhERUeVkpK8F3717FwUFBXBwcFArd3BwwM2bN4uc5ubNm0XWz8/Px927d1GjRg2NaXJzc5Gbmys9T09PB/AkGRJVFtxdiahSKYdOS/W9re2JKL0FHRXFM4dNhRAaZc+rX1S5SmRkJMLDwzXKnZ2dtW0qkd5YW+u7BUREWijHTiszMxPWWsxfb0HHzs4OhoaGGkdvbt++rXHURsXR0bHI+kZGRrC1tS1ymqlTp2LChAnS88LCQty/fx+2trYlBioqXxkZGXB2dsbVq1fLZawUEZEusc/SPyEEMjMz4eTkpNV0egs6JiYm8Pb2RkJCAnr27CmVJyQkoHv37kVO4+fnhz/++EOtbPv27fDx8YGxsXGR05iamsLU1FStrGrVqi/WeNIZKysrdhpEVGmwz9IvbY7kqOj18vIJEyZgyZIlWLZsGc6cOYPx48cjJSUFYWFhAJ4cjQkNDZXqh4WF4cqVK5gwYQLOnDmDZcuWYenSpZg0aZK+VoGIiIgqML2O0enXrx/u3buHiIgIpKamolGjRoiPj4erqysAIDU1Ve2eOu7u7oiPj8f48ePx448/wsnJCd9//z169eqlr1UgIiKiCkyv99GhV1dubi4iIyMxdepUjVOLREQVDfusyotBh4iIiGRL7791RURERFReGHSIiIhIthh0iIiISLYYdCqJPXv2QKFQ4MGDByXWc3Nzw/z5819Kmyq70m7Tsrh8+TIUCgWSk5N1Pm8iuXmV+zeFQoG4uLhymbcct1dZMOi8ZIsWLUKVKlWQn58vlWVlZcHY2BgBAQFqdffv3w+FQoH//vsP/v7+SE1NlW6WtHz5cr3e+FAOH6CKtk2JKju59G8vU2pqKjp37gyA/yCVFwadlywoKAhZWVk4evSoVLZ//344OjriyJEjePjwoVS+Z88eODk5oV69ejAxMYGjoyN/tkKHuE2JdIv9m/YcHR15uXo5Y9B5yerXrw8nJyfs2bNHKtuzZw+6d++O2rVr4+DBg2rlQUFB0t+qQ7t79uzB0KFDkZ6eDoVCAYVCgVmzZknTPXz4EMOGDUOVKlXg4uKCxYsXq7Xh1KlTaNu2LczMzGBra4uRI0ciKytLer1NmzYYN26c2jQ9evTAkCFDpNevXLmC8ePHS8svjkKhwE8//YSuXbvC3Nwcnp6eSExMxPnz59GmTRtYWFjAz88PFy5ckKa5cOECunfvDgcHB1haWqJ58+bYsWOH2nxTU1PRpUsXmJmZwd3dHStXrtQ4yqRQKLBkyRL07NkT5ubmqFu3LjZu3Ki2fUuzTYs6tFy1alUsX75cen748GE0bdoUSqUSPj4+SEpK0tgWp0+fRkhICCwtLeHg4IBBgwbh7t27xW47osqmIvRvz2rTpg1Gjx6NcePGwcbGBg4ODli8eDGys7MxdOhQVKlSBbVr18aWLVukaQoKCjB8+HC4u7vDzMwM9evXx3fffac23/z8fIwZMwZVq1aFra0tpkyZgsGDB6NHjx5qyx4zZgw++ugjVKtWDY6OjmrrAqj3L+7u7gCApk2bQqFQoE2bNtJ8SuqTgSe/+9itWzepT4yJidHYFunp6Rg5ciTs7e1hZWWFtm3b4sSJEyVuPzlg0NGDNm3aYPfu3dLz3bt3o02bNggMDJTKHz9+jMTERKkjeJq/vz/mz58PKysrpKamIjU1Ve1nML755hvpy/aDDz7A+++/j3///RfAk06iU6dOsLGxwZEjR7B27Vrs2LEDH374Yanbv379etSqVUu6o3VqamqJ9T/77DOEhoYiOTkZDRo0wMCBA/Hee+9h6tSp0n9+Ty8/KysLISEh2LFjB5KSktCxY0d069ZN7S7ZoaGhuHHjBvbs2YN169Zh8eLFuH37tsayw8PD0bdvX5w8eRIhISF4++23cf/+fa23aUmys7PRtWtX1K9fH8eOHcOsWbM0pk1NTUVgYCCaNGmCo0ePYuvWrbh16xb69u1bqmUQVRb67N+K88svv8DOzg6HDx/G6NGj8f7776NPnz7w9/fH8ePH0bFjRwwaNEg64lRYWIhatWphzZo1OH36NGbOnIlp06ZhzZo10jy/+uorxMTEIDo6GgcOHEBGRkaRY21++eUXWFhY4K+//sKcOXMQERGBhISEItt5+PBhAMCOHTuQmpqK9evXl7heTxsyZAguX76MXbt24bfffsPChQvV+kQhBLp06YKbN28iPj4ex44dQ7NmzdCuXbsi+0RZEfTSLV68WFhYWIi8vDyRkZEhjIyMxK1bt8Tq1auFv7+/EEKIvXv3CgDiwoULQgghdu/eLQCItLQ0IYQQ0dHRwtraWmPerq6u4p133pGeFxYWCnt7exEVFSUt28bGRmRlZUl1Nm/eLAwMDMTNmzeFEEIEBgaKsWPHqs23e/fuYvDgwWrLmTdv3nPXFYD45JNPpOeJiYkCgFi6dKlUtmrVKqFUKkucT8OGDcWCBQuEEEKcOXNGABBHjhyRXj937pwAoNamZ5edlZUlFAqF2LJlixCi9NsUgNiwYYNambW1tYiOjhZCCPHTTz+JatWqiezsbOn1qKgoAUAkJSUJIYSYMWOGCA4OVpvH1atXBQBx9uzZEtedqDLRZ/9WlMDAQNGqVSvpeX5+vrCwsBCDBg2SylJTUwUAkZiYWOx8PvjgA9GrVy/puYODg5g7d67afF1cXET37t2LXbYQQjRv3lxMmTJFev50/3Lp0iW1fuPp+ZTUJ589e1YAEIcOHZJeV/WTqj5x586dwsrKSjx69EhtPrVr1xY//fRTsestB3r9ratXVVBQELKzs3HkyBGkpaWhXr16sLe3R2BgIAYNGoTs7Gzs2bMHLi4u8PDw0Hr+jRs3lv5WKBRwdHSUkv2ZM2fw+uuvw8LCQqrTsmVLFBYW4uzZs3BwcHjxFSyhPar5e3l5qZU9evQIGRkZsLKyQnZ2NsLDw7Fp0ybcuHED+fn5yMnJkY7onD17FkZGRmjWrJk0jzp16sDGxqbEZVtYWKBKlSpFHvl5Eaptam5uLpX5+fmp1Tl27Bh2794NS0tLjekvXLiAevXq6bRNRPqiz/6tNNMYGhrC1tZWow8CoDafRYsWYcmSJbhy5QpycnLw+PFjNGnSBMCTU0C3bt1CixYt1Obr7e2NwsLCYpcNADVq1CiXPsjIyAg+Pj5SWYMGDdQGdB87dgxZWVmwtbVVmzYnJ0dt6IAcMejoQZ06dVCrVi3s3r0baWlpCAwMBPBkUJq7uzsOHDiA3bt3o23btmWav7GxsdpzhUIhffiEEMWOqVGVGxgYQDzzyyB5eXllasuz7VEto6gyVRsnT56Mbdu24euvv0adOnVgZmaG3r174/Hjx9I6FKWo8pK2RWkpFIoSt0dx7XlaYWEhunXrhq+++krjtRo1amjVHqKKTJ/9mzbTlNQHrVmzBuPHj8c333wDPz8/VKlSBXPnzsVff/2lMZ+nlVcf9Lw+WfVaSeMlCwsLUaNGDbXxUypyv8KNY3T0JCgoCHv27MGePXukAWcAEBgYiG3btuHQoUNFnr9WMTExQUFBgdbLbdiwIZKTk5GdnS2VHThwAAYGBtJRherVq6uNuykoKMDff/+tk+WXxv79+zFkyBD07NkTXl5ecHR0xOXLl6XXGzRogPz8fLUBv+fPn3/h++EUt07Pbo9z586pXT3SsGFDnDhxAjk5OVLZoUOH1ObRrFkz/PPPP3Bzc0OdOnXUHk8fXSOSA331b7qyf/9++Pv744MPPkDTpk1Rp04dtaMe1tbWcHBwkMbUAE/6yaIuQtCGiYmJNK+nPa9P9vT0RH5+vtrVbmfPnlXrE5s1a4abN2/CyMhIow+ys7N7oXZXdAw6ehIUFIQ///wTycnJ0n88wJOO4Oeff8ajR49K7Ajc3NyQlZWFnTt34u7du2pfvCV5++23oVQqMXjwYPz999/YvXs3Ro8ejUGDBkmHb9u2bYvNmzdj8+bN+Pfff/HBBx9ohAg3Nzfs27cP169f1/mVQ3Xq1MH69euRnJyMEydOYODAgWr/ATVo0ADt27fHyJEjcfjwYSQlJWHkyJEwMzN7octTi9umbdu2xQ8//IDjx4/j6NGjCAsLU/svbeDAgTAwMMDw4cNx+vRpxMfH4+uvv1ab96hRo3D//n0MGDAAhw8fxsWLF7F9+3YMGzZMrx06UXnQV/+mK3Xq1MHRo0exbds2/Pfff5gxYwaOHDmiVmf06NGIjIzE77//jrNnz2Ls2LFIS0t7oT7I3t4eZmZm0sUK6enpAJ7fJ9evXx+dOnXCu+++i7/++gvHjh3DiBEjYGZmJtVp3749/Pz80KNHD2zbtg2XL1/GwYMH8cknn6gFJDli0NGToKAg5OTkoE6dOmrjYgIDA5GZmYnatWvD2dm52On9/f0RFhaGfv36oXr16pgzZ06plmtubo5t27bh/v37aN68OXr37o127drhhx9+kOoMGzYMgwcPRmhoKAIDA+Hu7q7RKUVERODy5cuoXbs2qlevruXal2zevHmwsbGBv78/unXrho4dO6qNxwGAFStWwMHBAa1bt0bPnj3x7rvvokqVKlAqlWVebnHb9JtvvoGzszNat26NgQMHYtKkSWrjcSwtLfHHH3/g9OnTaNq0KaZPn65xisrJyQkHDhxAQUEBOnbsiEaNGmHs2LGwtraGgQE/hiQv+urfdCUsLAxvvfUW+vXrB19fX9y7dw8ffPCBWp0pU6ZgwIABCA0NhZ+fHywtLdGxY8cX6oOMjIzw/fff46effoKTkxO6d+8OoHR9cnR0NJydnREYGIi33npLuoxcRaFQID4+Hq1bt8awYcNQr1499O/fH5cvXy6XsZkViUKUZoABUQV37do1ODs7Y8eOHWjXrp2+m0NEr5jCwkJ4enqib9+++Oyzz/TdHHoKByNTpbRr1y5kZWXBy8sLqamp+Oijj+Dm5obWrVvru2lE9Aq4cuUKtm/fjsDAQOTm5uKHH37ApUuXMHDgQH03jZ7BoEOVUl5eHqZNm4aLFy+iSpUq8Pf3R0xMjMYVDkRE5cHAwADLly/HpEmTIIRAo0aNsGPHDnh6euq7afQMnroiIiIi2eIoSCIiIpItBh0iIiKSLQYdIiIiki0GHSIiIpItBh0ieqkUCgXi4uLKfTl79uyBQqFQu4NsXFwc6tSpA0NDQ4wbNw7Lly+X/e/8EL3qGHSISKdu3ryJ0aNHw8PDA6ampnB2dka3bt2wc+fOl9oOf39/pKamwtraWip777330Lt3b1y9ehWfffYZ+vXrh//++++ltouIXi7eR4eIdOby5cto2bIlqlatijlz5qBx48bIy8vDtm3bMGrUKPz7778vrS0mJiZwdHSUnmdlZeH27dvo2LEjnJycpPKnfw+oLPLy8nj/JqIKjEd0iEhnPvjgAygUChw+fBi9e/dGvXr18Nprr2HChAkav+iuMmXKFNSrVw/m5ubw8PDAjBkzkJeXJ71+4sQJBAUFoUqVKrCysoK3t7f0I4RXrlxBt27dYGNjAwsLC7z22muIj48HoH7qas+ePahSpQqAJz+QqFAosGfPniJPXf3xxx/w9vaGUqmEh4cHwsPDkZ+fL72uUCiwaNEidO/eHRYWFpg9ezbS0tLw9ttvo3r16jAzM0PdunURHR2ty01LRGXEIzpEpBP379/H1q1b8fnnn8PCwkLj9eLGwlSpUgXLly+Hk5MTTp06Jf1A60cffQQAePvtt9G0aVNERUXB0NAQycnJ0hGUUaNG4fHjx9i3bx8sLCxw+vRpWFpaaizD398fZ8+eRf369bFu3Tr4+/ujWrVquHz5slq9bdu24Z133sH333+PgIAAXLhwASNHjgQAfPrpp1K9Tz/9FJGRkZg3bx4MDQ0xY8YMnD59Glu2bIGdnR3Onz+PnJycsmxGItIxBh0i0onz589DCIEGDRpoNd0nn3wi/e3m5oaJEyciNjZWCjopKSmYPHmyNN+6detK9VNSUtCrVy94eXkBADw8PIpchomJifRLztWqVVM7pfW0zz//HB9//DEGDx4sze+zzz7DRx99pBZ0Bg4ciGHDhqm1o2nTpvDx8ZHWg4gqBgYdItIJ1a/JKBQKrab77bffMH/+fJw/fx5ZWVnIz8+HlZWV9PqECRMwYsQI/Prrr2jfvj369OmD2rVrAwDGjBmD999/H9u3b0f79u3Rq1cvNG7cuMzrcOzYMRw5cgSff/65VFZQUIBHjx7h4cOHMDc3BwAp0Ki8//776NWrF44fP47g4GD06NED/v7+ZW4HEekOx+gQkU7UrVsXCoUCZ86cKfU0hw4dQv/+/dG5c2ds2rQJSUlJmD59Oh4/fizVmTVrFv755x906dIFu3btQsOGDbFhwwYAwIgRI3Dx4kUMGjQIp06dgo+PDxYsWFDmdSgsLER4eDiSk5Olx6lTp3Du3DkolUqp3rOn5jp37owrV65g3LhxuHHjBtq1a4dJkyaVuR1EpDsMOkSkE9WqVUPHjh3x448/Ijs7W+P1p+9no3LgwAG4urpi+vTp8PHxQd26dXHlyhWNevXq1cP48eOxfft2vPXWW2oDfZ2dnREWFob169dj4sSJ+Pnnn8u8Ds2aNcPZs2dRp04djYeBQcndZfXq1TFkyBD873//w/z587F48eIyt4OIdIenrohIZxYuXAh/f3+0aNECERERaNy4MfLz85GQkICoqCiNoz116tRBSkoKVq9ejebNm2Pz5s3S0RoAyMnJweTJk9G7d2+4u7vj2rVrOHLkCHr16gUAGDduHDp37ox69eohLS0Nu3btgqenZ5nbP3PmTHTt2hXOzs7o06cPDAwMcPLkSZw6dQqzZ88ucTpvb2+89tpryM3NxaZNm16oHUSkOzyiQ0Q64+7ujuPHjyMoKAgTJ05Eo0aN0KFDB+zcuRNRUVEa9bt3747x48fjww8/RJMmTXDw4EHMmDFDet3Q0BD37t1DaGgo6tWrh759+6Jz584IDw8H8GT8zKhRo+Dp6YlOnTqhfv36WLhwYZnb37FjR2zatAkJCQlo3rw53njjDXz77bdwdXUtcToTExNMnToVjRs3RuvWrWFoaIjVq1eXuR1EpDsKoRpBSERERCQzPKJDREREssWgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESyxaBDREREssWgQ0RERLLFoENERESy9f8AAb45kWRsCCsAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure()\n",
        "plt.bar(['Without magnitude', 'With magnitude'], [token_accuracy, magnitude_accuracy], \n",
        "        width=0.5, color=['b', 'r'])\n",
        "plt.xlabel('Classifiers')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Sentiment classifier accuracy (threshold=8., magnitude=2.)')\n",
        "plt.ylim(ymax = 1, ymin = 0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNhS8OCVxMHd"
      },
      "source": [
        "#### (Q1.4) A better threshold (1pt)\n",
        "Above we have defined a threshold to account for an inherent bias in the dataset: there are more positive than negative words per review.\n",
        "However, that threshold does not take into account *document length*. Explain why this is a problem and implement an alternative way to compute the threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo7gk1I-omLI"
      },
      "source": [
        "The threshold defined is *absolute* but we need to account for document length. As positive words occur at a higher frequency than negative words, longer documents will have greater abolute positive bias than shorter ones. One solution is to adjust the threshold according to the document size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 536,
      "metadata": {
        "id": "Dwt0B8h8aKjr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Avg bias per word in dataset = 0.2278\n",
            "Accuracy of sentiment classifer (with magnitude and relative threshold): 0.694\n"
          ]
        }
      ],
      "source": [
        "# Get avg bias for entire dataset\n",
        "def get_bias_per_word(corpus):\n",
        "    '''\n",
        "    Get the average bias per word in the dataset.\n",
        "    '''\n",
        "    score = 0\n",
        "    n_words = 0\n",
        "    polarity_score = {'positive': 1, 'negative': -1, 'neutral': 0, 'both': 0}\n",
        "    for doc in corpus:\n",
        "        for sentence in doc['content']:\n",
        "            for word, pos_tag in sentence:\n",
        "                word = word.lower() # convert to lowercase\n",
        "                if word in sent_lexicon.keys():\n",
        "                    score += polarity_score[sent_lexicon[word]['polarity']]\n",
        "            n_words += 1\n",
        "    bias_per_word = score/n_words\n",
        "    \n",
        "    return bias_per_word\n",
        "\n",
        "bias_per_word = get_bias_per_word(reviews)\n",
        "print(f'Avg bias per word in dataset = {bias_per_word:1.4f}')\n",
        "\n",
        "# Classify reviews using relative threshold and magnitude\n",
        "sent_preds_magnitude_rel = pred_sentiment(reviews, sent_lexicon, magnitude_multiplier=2., threshold='relative', bias_per_word=bias_per_word)\n",
        "magnitude_rel_results = (np.array(sent_preds_magnitude_rel) == np.array(sent_labels)).astype(int)\n",
        "magnitude_rel_accuracy = np.mean(magnitude_rel_results)\n",
        "print(f'Accuracy of sentiment classifer (with magnitude and relative threshold): {magnitude_rel_accuracy:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LibV4nR89BXb"
      },
      "source": [
        "# (2) Naive Bayes (9.5pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnF9adQnuwia"
      },
      "source": [
        "\n",
        "Your second task is to program a simple Machine Learning approach that operates\n",
        "on a simple Bag-of-Words (BoW) representation of the text data, as\n",
        "described by Pang et al. (2002). In this approach, the only features we\n",
        "will consider are the words in the text themselves, without bringing in\n",
        "external sources of information. The BoW model is a popular way of\n",
        "representing texts as vectors, making it\n",
        "easy to apply classical Machine Learning algorithms on NLP tasks.\n",
        "However, the BoW representation is also very crude, since it discards\n",
        "all information related to word order and grammatical structure in the\n",
        "original text—as the name suggests.\n",
        "\n",
        "## Writing your own classifier (4pts)\n",
        "\n",
        "Write your own code to implement the Naive Bayes (NB) classifier. As\n",
        "a reminder, the Naive Bayes classifier works according to the following\n",
        "equation:\n",
        "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} P(c|\\bar{f}) = \\operatorname*{arg\\,max}_{c \\in C} P(c)\\prod^n_{i=1} P(f_i|c)$$\n",
        "where $C = \\{ \\text{POS}, \\text{NEG} \\}$ is the set of possible classes,\n",
        "$\\hat{c} \\in C$ is the most probable class, and $\\bar{f}$ is the feature\n",
        "vector. Remember that we use the log of these probabilities when making\n",
        "a prediction:\n",
        "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} \\Big\\{\\log P(c) + \\sum^n_{i=1} \\log P(f_i|c)\\Big\\}$$\n",
        "\n",
        "You can find more details about Naive Bayes in [Jurafsky &\n",
        "Martin](https://web.stanford.edu/~jurafsky/slp3/). You can also look at\n",
        "this helpful\n",
        "[pseudo-code](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html).\n",
        "\n",
        "*Note: this section and the next aim to put you in a position to replicate\n",
        "    Pang et al.'s Naive Bayes results. However, your numerical results\n",
        "    will differ from theirs, as they used different data.*\n",
        "\n",
        "**You must write the Naive Bayes training and prediction code from\n",
        "scratch.** You will not be given credit for using off-the-shelf Machine\n",
        "Learning libraries.\n",
        "\n",
        "The data contains the text of the reviews, where each document consists\n",
        "of the sentences in the review, the sentiment of the review and an index\n",
        "(cv) that you will later use for cross-validation. The\n",
        "text has already been tokenised and POS-tagged for you. Your algorithm\n",
        "should read in the text, **lowercase it**, store the words and their\n",
        "frequencies in an appropriate data structure that allows for easy\n",
        "computation of the probabilities used in the Naive Bayes algorithm, and\n",
        "then make predictions for new instances.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEpyQSBSkb33"
      },
      "source": [
        "#### (Q2.1) Unseen words (1pt)\n",
        "The presence of words in the test dataset that\n",
        "have not been seen during training can cause probabilities in the Naive Bayes classifier to equal $0$.\n",
        "These can be words which are unseen in both positive and negative training reviews (case 1), but also words which are seen in reviews _of only one sentiment class_ in the training dataset (case 2). In both cases, **you should skip these words for both classes**.  What would be the problem instead with skipping words only for one class in case 2? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BanFiYYnoxDW"
      },
      "source": [
        "The marginal contribution of word $w$ to log-cond_prob is $\\log P(w|C) \\in (-\\infty, 0)$ (as $P(w|C) \\in (0, 1)$). If you skip the word for the class in which it *does not occur*, you add 0 to the log-likehood for that class, and some negative value for the other class, which actually *decreases* the cond_prob for the class in which *does occur*, which is obviously nonsensical. And instead, if we include it only for the class in which it *does not occur*, then the likehood for that class (or joint probability) becomes 0, as it is a product of the individual probabilities and the probability for that class is 0. But this is also too extreme. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsZRhaI3WvzC"
      },
      "source": [
        "#### (Q2.2) Train your classifier on (positive and negative) reviews with cv-value 000-899, and test it on the remaining (positive and negative) reviews cv900–cv999.  Report results using classification accuracy as your evaluation metric. Your  features are the word vocabulary. The value of a feature is the count of that feature (word) in the document. (2pts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 537,
      "metadata": {
        "id": "GWDkt5ZrrFGp"
      },
      "outputs": [],
      "source": [
        "# Create training and test sets\n",
        "reviews_train = [r for r in reviews if r['cv'] < 900]\n",
        "reviews_test = [r for r in reviews if r['cv'] >= 900 and r['cv'] < 1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 538,
      "metadata": {
        "id": "G7zaJYGFvIJ3"
      },
      "outputs": [],
      "source": [
        "def get_unseen_words(reviews):\n",
        "    '''\n",
        "    Get a list of words in the training set which only occur in one class.\n",
        "    Inputs:\n",
        "        reviews - a list of reviews in the training set\n",
        "    Output:\n",
        "        unseen_words - a list of words which only occur in one class\n",
        "    '''\n",
        "    pos_words = set()\n",
        "    neg_words = set()\n",
        "    for review in reviews:\n",
        "        for sentence in review['content']:\n",
        "            for word, pos_tag in sentence:\n",
        "                word = word.lower() # convert to lowercase\n",
        "                # if word in sent_lexicon.keys():\n",
        "                if review['sentiment'] == 'POS':\n",
        "                    pos_words.add(word)\n",
        "                else:\n",
        "                    neg_words.add(word)\n",
        "    unseen_words = pos_words ^ neg_words\n",
        "    \n",
        "    return unseen_words\n",
        "\n",
        "unseen_words = get_unseen_words(reviews_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 539,
      "metadata": {},
      "outputs": [],
      "source": [
        "stem_dict = {}  # global dict of already stemmed words\n",
        "def extract_features(content, smoothing=0., unseen_words=unseen_words, stem=False, n_grams=[1]):\n",
        "    '''\n",
        "    Extract features as ngrams from corpus.\n",
        "    Inputs:\n",
        "        content - a list of sentences, where a sentence is a list of (token, pos_tag) pairs\n",
        "        smoothing - smoothing parameter for unseen words\n",
        "        unseen_words - a set of words which only occur in one class\n",
        "        stem - whether to stem the tokens\n",
        "        n_grams - a list of n for n-grams\n",
        "    Output:\n",
        "        features - an iterator of ngrams\n",
        "    '''\n",
        "    stemmer = PorterStemmer() if stem else None\n",
        "    features = []\n",
        "    for sentence in content:\n",
        "        # preprocess the sentence\n",
        "        sentence_preprocessed = []\n",
        "        for word, pos_tag in sentence:\n",
        "            word = word.lower() # convert to lowercase\n",
        "            # remove unseen words if smoothing is not used\n",
        "            if not smoothing and word in unseen_words:\n",
        "                continue\n",
        "            if stem:\n",
        "                if word in stem_dict:\n",
        "                    word = stem_dict[word]\n",
        "                else:\n",
        "                    word = stemmer.stem(word)\n",
        "                    stem_dict[word] = word\n",
        "            sentence_preprocessed.append(word)\n",
        "        # add all n_grams (from n_grams) to the features\n",
        "        for n in n_grams:\n",
        "            features += ngrams(sentence_preprocessed, n, pad_left=True, pad_right=True)\n",
        "\n",
        "    return Counter(features)\n",
        "\n",
        "\n",
        "def train_NB(reviews_train, unseen_words=unseen_words, smoothing=0., stem=False, n_grams=[1]):\n",
        "    '''\n",
        "    Train a Naive Bayes classifier.\n",
        "    Inputs:\n",
        "        reviews_train - a list of reviews in the training set\n",
        "        smoothing - smoothing parameter for unseen words\n",
        "        stem - whether to stem the tokens\n",
        "        n_grams - a list of n for n-grams\n",
        "    Output:\n",
        "        prior - a dictionary mapping class labels to prior probabilities\n",
        "        cond_prob - a dictionary mapping class labels to dictionaries mapping features to conditional probabilities\n",
        "    '''\n",
        "    priors = defaultdict(lambda: 0)\n",
        "    cond_probs = defaultdict(lambda: Counter())\n",
        "\n",
        "    for r in reviews_train:\n",
        "        priors[r['sentiment']] += 1\n",
        "        features = extract_features(r[\"content\"], unseen_words=unseen_words, smoothing=smoothing, stem=stem, n_grams=n_grams)\n",
        "        cond_probs[r['sentiment']] += features\n",
        "\n",
        "    # normalize priors\n",
        "    N_doc = sum(priors.values())\n",
        "    for sentiment in priors:\n",
        "        priors[sentiment] /= N_doc\n",
        "\n",
        "    # normalize cond_probs\n",
        "    vocabulary = list(sum(cond_probs.values(), Counter()).keys())\n",
        "    for sentiment in cond_probs:\n",
        "        normalization = sum(cond_probs[sentiment].values()) + len(vocabulary) * smoothing\n",
        "        for feature in vocabulary:\n",
        "            cond_probs[sentiment][feature] = (cond_probs[sentiment][feature] + smoothing) / normalization\n",
        "        \n",
        "    \n",
        "    return priors, cond_probs\n",
        "\n",
        "\n",
        "def NB_classifier(reviews_test, priors, cond_probs, smoothing=0., stem=False, n_grams=[1]):\n",
        "    '''\n",
        "    Classify reviews using a Naive Bayes classifier.\n",
        "    Inputs:\n",
        "        reviews_test - a list of reviews in the test set\n",
        "        priors - a dictionary mapping class labels to prior probabilities\n",
        "        cond_probs - a dictionary mapping class labels to dictionaries mapping features to conditional probabilities\n",
        "        stem - whether to stem the tokens\n",
        "        n_grams - a list of n for n-grams\n",
        "    Output:\n",
        "        predictions - a list of predicted class labels\n",
        "    '''\n",
        "    predictions = []\n",
        "    for r in reviews_test:\n",
        "        features = extract_features(r[\"content\"], smoothing=smoothing, stem=stem, n_grams=n_grams)\n",
        "        scores = {sentiment: math.log(priors[sentiment]) for sentiment in priors}\n",
        "        for feature in features:\n",
        "            if cond_probs['NEG'][feature] != 0 and cond_probs['POS'][feature] != 0:\n",
        "                for sentiment in scores:\n",
        "                    scores[sentiment] += math.log(cond_probs[sentiment][feature]) * features[feature]\n",
        "        \n",
        "        predictions.append(max(scores, key=scores.get))\n",
        "    \n",
        "    return predictions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 540,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of NB classifier: 0.835\n"
          ]
        }
      ],
      "source": [
        "# Evaluate NB on the test set\n",
        "priors, cond_probs = train_NB(reviews_train)\n",
        "predictions_NB = NB_classifier(reviews_test, priors, cond_probs)\n",
        "labels_test = [r['sentiment'] for r in reviews_test]\n",
        "NB_accuracy = (np.array(labels_test) == np.array(predictions_NB)).mean()\n",
        "print(f'Accuracy of NB classifier: {NB_accuracy:1.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0INK-PBoM6CB"
      },
      "source": [
        "#### (Q2.3) Would you consider accuracy to also be a good way to evaluate your classifier in a situation where 90% of your data instances are of positive movie reviews? (1pt)\n",
        "\n",
        "Simulate this scenario by keeping the positive reviews\n",
        "data unchanged, but only using negative reviews cv000–cv089 for\n",
        "training, and cv900–cv909 for testing. Calculate the classification\n",
        "accuracy, and explain what changed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Class imbalance in the training set is generally undesirable, as the model can be biased toward the dominant class. In an extreme case, the model could only predict the dominant class for all cases. If class imbalance is also present in the test set, accuracy is not a good metric. For example, if the test set has 90\\% occurence of the positive class, a naive model which always predicts the positive class gets a 90\\% accuracy. But this is not a good metric as the model had not learnt anything and has 0 accuracy/precision/recall on the negative class. In our case, since the training set contains mostly positive reviews, the algorithm learns to predict the postitive class almost all the time. So it achieves a high accuracy (90\\%) on the test set, as it has mostly positive reviews as well. But in fact it performs very poorly on the negative class and would not generalize well to other data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 541,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The fraction of negative reviews in the imbalanced train set is 0.09\n",
            "The fraction of negative reviews in the test set using NB is: 0.08\n",
            "The fraction of negative reviews in the predictions using NB is 0.02: \n",
            "Accuracy of NB classifier on imbalanced train/test sets: 0.90\n"
          ]
        }
      ],
      "source": [
        "# Train and test on imbalanced sets\n",
        "# create imbalanced train set\n",
        "reviews_train_imbalanced = [r for r in reviews_train if r['sentiment'] == 'POS'] + [r for r in reviews_train if r['sentiment'] == 'NEG' and r['cv'] < 90]\n",
        "print('The fraction of negative reviews in the imbalanced train set is %1.2f' % (sum([r['sentiment'] == 'NEG' for r in reviews_train_imbalanced])/len(reviews_train_imbalanced)))\n",
        "# create imbalanced test set\n",
        "reviews_test_imbalanced = [r for r in reviews_test if r['sentiment'] == 'POS'] + [r for r in reviews_test if r['sentiment'] == 'NEG' and r['cv'] > 900 and r['cv'] < 910]\n",
        "labels_test_imbalanced = [r['sentiment'] for r in reviews_test_imbalanced]\n",
        "print('The fraction of negative reviews in the test set using NB is: %.2f' % (labels_test_imbalanced.count('NEG')/len(labels_test_imbalanced)))\n",
        "# Evaluate on imbalanced test set\n",
        "# get unseen words\n",
        "unseen_words_imbalanced = get_unseen_words(reviews_train_imbalanced)\n",
        "priors, cond_probs = train_NB(reviews_train_imbalanced, unseen_words=unseen_words_imbalanced)\n",
        "predictions_NB_imbalanced = NB_classifier(reviews_test_imbalanced, priors, cond_probs)\n",
        "NB_accuracy_imbalanced = (np.array(labels_test_imbalanced) == np.array(predictions_NB_imbalanced)).mean()\n",
        "print('The fraction of negative reviews in the predictions using NB is %.2f: ' % (predictions_NB_imbalanced.count('NEG')/len(predictions_NB_imbalanced)))\n",
        "print(\"Accuracy of NB classifier on imbalanced train/test sets: %1.2f\" % NB_accuracy_imbalanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Smoothing (1pt)\n",
        "\n",
        "As mentioned above, the presence of words in the test dataset that\n",
        "have not been seen during training can cause probabilities in the Naive\n",
        "Bayes classifier to be $0$, thus making that particular test instance\n",
        "undecidable. The standard way to mitigate this effect (as well as to\n",
        "give more clout to rare words) is to use smoothing, in which the\n",
        "probability fraction\n",
        "$$\\frac{\\text{count}(w_i, c)}{\\sum\\limits_{w\\in V} \\text{count}(w, c)}$$ \n",
        "for a word\n",
        "$w_i$ becomes\n",
        "$$\\frac{\\text{count}(w_i, c) + \\text{smoothing}(w_i)}{\\sum\\limits_{w\\in V} \\text{count}(w, c) + \\sum\\limits_{w \\in V} \\text{smoothing}(w)}$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBNIcbwUWphC"
      },
      "source": [
        "#### (Q2.4) Implement Laplace feature smoothing (1pt)\n",
        "Implement Laplace smoothing, i.e., smoothing with a constant value ($smoothing(w) = \\kappa, \\forall w \\in V$), in your Naive\n",
        "Bayes classifier’s code, and report the impact on performance. \n",
        "Use $\\kappa = 1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 542,
      "metadata": {
        "id": "g03yflCc9kpW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of NB classifier with smoothing: 0.825\n"
          ]
        }
      ],
      "source": [
        "# NB classifier with smoothing\n",
        "priors, cond_probs = train_NB(reviews_train, smoothing=1.)  # laplace smoothing with k=1\n",
        "predictions_NB_smooth = NB_classifier(reviews_test, priors, cond_probs, smoothing=1.)\n",
        "nb_acc_smooth = (np.array(labels_test) == np.array(predictions_NB_smooth)).mean()\n",
        "print(f'Accuracy of NB classifier with smoothing: {nb_acc_smooth:1.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performance remains roughly the same after applying smoothing (test accuracy of ~82%). In the earlier case we were discarding unseen words, now we consider all words with smoothing. It does not appear to make much difference which tenchique we use in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiGcgwba87D5"
      },
      "source": [
        "## Cross-Validation (1.5pts)\n",
        "\n",
        "A serious danger in using Machine Learning on small datasets, with many\n",
        "iterations of slightly different versions of the algorithms, is ending up with Type III errors, also called the “testing hypotheses\n",
        "suggested by the data” errors. This type of error occurs when we make\n",
        "repeated improvements to our classifiers by playing with features and\n",
        "their processing, but we don’t get a fresh, never-before seen test\n",
        "dataset every time. Thus, we risk developing a classifier that gets better\n",
        "and better on our data, but only gets worse at generalizing to new, unseen data. In other words, we risk developping a classifier that overfits.\n",
        "\n",
        "A simple method to guard against Type III errors is to use\n",
        "Cross-Validation. In **N-fold Cross-Validation**, we divide the data into N\n",
        "distinct chunks, or folds. Then, we repeat the experiment N times: each\n",
        "time holding out one of the folds for testing, training our classifier\n",
        "on the remaining N - 1 data folds, and reporting performance on the\n",
        "held-out fold. We can use different strategies for dividing the data:\n",
        "\n",
        "-   Consecutive splitting:\n",
        "  - cv000–cv099 = Split 1\n",
        "  - cv100–cv199 = Split 2\n",
        "  - etc.\n",
        "  \n",
        "-   Round-robin splitting (mod 10):\n",
        "  - cv000, cv010, cv020, … = Split 1\n",
        "  - cv001, cv011, cv021, … = Split 2\n",
        "  - etc.\n",
        "\n",
        "-   Random sampling/splitting\n",
        "  - Not used here (but you may choose to split this way in a non-educational situation)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OeLcbSauGtR"
      },
      "source": [
        "#### (Q2.5) Write the code to implement 10-fold cross-validation using round-robin splitting for your Naive Bayes classifier from Q2.4 and compute the 10 accuracies. Report the final performance, which is the average of the performances per fold. If all splits perform equally well, this is a good sign. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 516,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement K-fold cross-validation for the NB classifier with rounding-robin splitting\n",
        "def kfold_cv_nb(reviews, smoothing=0., stem=False, n_grams=[1], n_folds=10):\n",
        "    '''\n",
        "    Perform K-fold cross-validation for the NB classifier.\n",
        "    Inputs:\n",
        "        reviews - a list of reviews in the train set\n",
        "        smoothing - the smoothing parameter\n",
        "        stem - whether to stem the tokens\n",
        "        n_grams - a list of n for n-grams\n",
        "        n_folds - the number of CV folds\n",
        "    Output:\n",
        "        mean_accuracy - the average accuracy of the classifier over the folds\n",
        "        var_accuracy - the variance of the accuracy of the classifier over the folds\n",
        "    '''\n",
        "    accuracies = []\n",
        "    for k in tqdm(range(n_folds)):\n",
        "        # split the data into train and validation sets using round-robin splitting\n",
        "        reviews_val = reviews[k::n_folds]\n",
        "        reviews_train = [r for r in reviews if r not in reviews_val]\n",
        "        # train the model\n",
        "        priors, cond_probs = train_NB(reviews_train, smoothing=smoothing, stem=stem, n_grams=n_grams)\n",
        "        predictions_NB = NB_classifier(reviews_val, priors, cond_probs, smoothing=smoothing, stem=stem, n_grams=n_grams)\n",
        "        labels_val = [r['sentiment'] for r in reviews_val]\n",
        "        accuracies.append((np.array(labels_val) == np.array(predictions_NB)).mean())\n",
        "    mean_accuracy = sum(accuracies) / len(accuracies)\n",
        "    var_accuracy = sum([(a - mean_accuracy)**2 for a in accuracies]) / len(accuracies)\n",
        "    \n",
        "    return mean_accuracy, var_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 517,
      "metadata": {
        "id": "ZoBQm1KuNzNR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:18<00:00,  1.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average accuracy of NB classifier with 10-fold CV: 0.818\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "nb_acc_mean_cv, nb_acc_var_cv = kfold_cv_nb(reviews_train, smoothing=1., stem=False, n_folds=10)\n",
        "print(f'Average accuracy of NB classifier with 10-fold CV: {nb_acc_mean_cv:1.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otdlsDXBNyOa"
      },
      "source": [
        "#### (Q2.6) Report the variance of the 10 accuracy scores. (0.5pt)\n",
        "\n",
        "**Please report all future results using 10-fold cross-validation now\n",
        "(unless told to use the held-out test set).** Note: you're not allowed to use a library for computing the variance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 518,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variance of accuracy of NB classifier with 10-fold CV: 0.00063\n"
          ]
        }
      ],
      "source": [
        "print(f'Variance of accuracy of NB classifier with 10-fold CV: {nb_acc_var_cv:1.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6A2zX9_BRKm"
      },
      "source": [
        "## Features, overfitting, and the curse of dimensionality\n",
        "\n",
        "In the Bag-of-Words model, ideally we would like each distinct word in\n",
        "the text to be mapped to its own dimension in the output vector\n",
        "representation. However, real world text is messy, and we need to decide\n",
        "on what we consider to be a word. For example, is “`word`\" different\n",
        "from “`Word`\", from “`word`”, or from “`words`\"? Too strict a\n",
        "definition, and the number of features explodes, while our algorithm\n",
        "fails to learn anything generalisable. Too lax, and we risk destroying\n",
        "our learning signal. In the following section, you will learn about\n",
        "confronting the feature sparsity and the overfitting problems as they\n",
        "occur in NLP classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKK8FNt8VtcZ"
      },
      "source": [
        "### Stemming (1.5pts)\n",
        "\n",
        "To make your algorithm more robust, use stemming and hash different inflections of a word to the same feature in the BoW vector space. Please use the [Porter stemming\n",
        "    algorithm](http://www.nltk.org/howto/stem.html) from NLTK.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "NxtCul1IrBi_"
      },
      "outputs": [],
      "source": [
        "# See train_NB, classifier_NB and extract_features above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SrJ1BeLXTnk"
      },
      "source": [
        "#### (Q2.7): How does the performance of your classifier change when you use stemming on your training and test datasets? (1pt)\n",
        "Use cross-validation to evaluate the classifier. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 519,
      "metadata": {
        "id": "gYqKBOiIrInT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [01:06<00:00,  6.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average accuracy of NB classifier (with stemming) using 10-fold CV: 0.812\n",
            "Variance of accuracy of NB classifier (with stemming) using 10-fold CV: 0.00095\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "nb_acc_stem_mean_cv, nb_acc_stem_var_cv = kfold_cv_nb(reviews_train, smoothing=1., stem=True, n_folds=10)\n",
        "print(f'Average accuracy of NB classifier (with stemming) using 10-fold CV: {nb_acc_stem_mean_cv:1.3f}')\n",
        "print(f'Variance of accuracy of NB classifier (with stemming) using 10-fold CV: {nb_acc_stem_var_cv:1.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The performance is roughly the same when applying stemming (~82% CV accuracy over 10 folds). The algorithm takes slighty longer when running for the first time, as it has to stem all the words in the vocabulary. But the tradeoff is that we can use a smaller vocabulary, thus achieving compression/dimensionality reduction without much information loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkDHVq_1XUVP"
      },
      "source": [
        "#### (Q2.8) What happens to the number of features (i.e., the size of the vocabulary) when using stemming as opposed to (Q2.4)? (0.5pt)\n",
        "Give actual numbers. You can use the held-out training set to determine these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 521,
      "metadata": {
        "id": "MA3vee5-rJyy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The size of the vocabulary without stemming: 45348\n",
            "The size of the vocabulary with stemming: 32332\n"
          ]
        }
      ],
      "source": [
        "vocab_unstemmed, vocab_stemmed = Counter(), Counter()\n",
        "for r in reviews_train:\n",
        "    vocab_unstemmed += extract_features(r[\"content\"], smoothing=1., stem=False)\n",
        "    vocab_stemmed += extract_features(r[\"content\"], smoothing=1., stem=True)\n",
        "print(f'The size of the vocabulary without stemming: {len(vocab_unstemmed)}')\n",
        "print(f'The size of the vocabulary with stemming: {len(vocab_stemmed)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The size of the vocabulary drops significantly when applying stemming, as various inflections of the root collapse to the same stem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoazfxbNV5Lq"
      },
      "source": [
        "### N-grams (1.5pts)\n",
        "\n",
        "A simple way of retaining some of the word\n",
        "order information when using bag-of-words representations is to use **n-gram** features. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHjy3I7-qWiu"
      },
      "source": [
        "#### (Q2.9) Retrain your classifier from (Q2.4) using **unigrams+bigrams** and **unigrams+bigrams+trigrams** as features. (1pt)\n",
        "Report accuracy and compare it with that of the approaches you have previously implemented. You are allowed to use NLTK to build n-grams from sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 522,
      "metadata": {
        "id": "eYuKMTOpq9jz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average accuracy of NB classifier (with stemming) over 10 folds with unigrams: 0.825\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [02:55<00:00, 17.53s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average accuracy of NB classifier (with stemming) over 10 folds, using unigrams+bigrams: 0.827\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [06:45<00:00, 40.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average accuracy of NB classifier (with stemming) over 10 folds, using unigrams+bigrams+trigrams: 0.835\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Compare different ngram models with K-fold CV\n",
        "print(f'Average accuracy of NB classifier (with stemming) over 10 folds with unigrams: {nb_acc_smooth:1.3f}')\n",
        "# print(f'Variance of accuracy of NB classifier (with stemming) over 10 folds, using unigrams: {np.var(nb_acc_smooth):1.5f}')\n",
        "# NB classifier with unigrams+bigrams\n",
        "nb_stem_ub_acc_cv, _ = kfold_cv_nb(reviews, smoothing=0., stem=True, n_grams=[1, 2], n_folds=10)\n",
        "print(f'Average accuracy of NB classifier (with stemming) over 10 folds, using unigrams+bigrams: {nb_stem_ub_acc_cv:1.3f}')\n",
        "# NB classifier with unigrams+bigrams+trigrams\n",
        "nb_stem_ubt_acc_cv, _ = kfold_cv_nb(reviews, smoothing=0., stem=True, n_grams=[1, 2, 3], n_folds=10)\n",
        "print(f'Average accuracy of NB classifier (with stemming) over 10 folds, using unigrams+bigrams+trigrams: {nb_stem_ubt_acc_cv:1.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generally, using more ngrams results in a richer model with more contextual understanding (as we can capture semantic relationships between phrases of words), which allows us to better understand the document's context. The prolem is that this is computationally expensive, as the complxeity grows as $O(n^k)$ where n is the vocabulary length and $k$ is the largest ngram. Moreover, we also need the much larger and richer dataset, as otherwise we would end up with very sparse models (as most of these higher ngrams occur rarely). \n",
        "\n",
        "This seems to be the case here. The accuracy remains roughly the same as before (around 83\\%), when using unigrams+trigrams, and unigrams+bigrams+trigrams. But the computational complexity (both time and space) grows exponentially with the number of ngrams. This could be because the dataset is too small for using bigrams and trigrams, and as such they don't contribute much useful informaion. We would be better off simply using unigrams without any loss of performance (and vastly improving computational efficiency)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVrGGArkrWoL"
      },
      "source": [
        "\n",
        "#### Q2.10: How many features does the BoW model have to take into account now? (0.5pt)\n",
        "How would you expect the number of features to increase theoretically (e.g., linear, square, cubed, exponential)? How does this number compare, in practice, to the number of features at (Q2.8)?\n",
        "\n",
        "Use the held-out training set once again for this.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 523,
      "metadata": {
        "id": "_z8sAJeUrdtM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of features in the vocabulary with unigrams: 32334\n",
            "The number of features in the vocabulary with unigrams+bigrams: 412827\n",
            "The number of features in the vocabulary with unigrams+bigrams+trigrams: 1306759\n"
          ]
        }
      ],
      "source": [
        "vocab_u, vocab_ub, vocab_ubt = Counter(), Counter(), Counter()\n",
        "for r in reviews_train:\n",
        "    vocab_u += extract_features(r['content'], smoothing=1., stem=True, n_grams=[1])\n",
        "    vocab_ub += extract_features(r['content'], smoothing=1., stem=True, n_grams=[1, 2])\n",
        "    vocab_ubt += extract_features(r['content'], smoothing=1., stem=True, n_grams=[1, 2, 3])\n",
        "print(f'The number of features in the vocabulary with unigrams: {len(vocab_u)}')\n",
        "print(f'The number of features in the vocabulary with unigrams+bigrams: {len(vocab_ub)}')\n",
        "print(f'The number of features in the vocabulary with unigrams+bigrams+trigrams: {len(vocab_ubt)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For a document with $n$ words, the number of unigrams equals $n$. In the worst case, the number of bigrams is $n^2$, if every word pairs with every other word in the vocabulary including itself. For trigrams, this becomes $n^3$. Although the worst case is highly unlikely because of grammar, in theory this number grows in order of polynomial time. For our case we observe that on increasing the number of ngrams, the vocabulary size increases by an order of magnitude each time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHWKDL3YV6vh"
      },
      "source": [
        "# (3) Support Vector Machines (4pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJSYhcVaoJGt"
      },
      "source": [
        "Though simple to understand, implement, and debug, one\n",
        "major problem with the Naive Bayes classifier is that its performance\n",
        "deteriorates (becomes skewed) when it is being used with features which\n",
        "are not independent (i.e., are correlated). Another popular classifier\n",
        "that doesn’t scale as well to big data, and is not as simple to debug as\n",
        "Naive Bayes, but that doesn’t assume feature independence is the Support\n",
        "Vector Machine (SVM) classifier.\n",
        "\n",
        "You can find more details about SVMs in Chapter 7 of Bishop: Pattern Recognition and Machine Learning.\n",
        "Other sources for learning SVM:\n",
        "* http://web.mit.edu/zoya/www/SVM.pdf\n",
        "* http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf\n",
        "* https://pythonprogramming.net/support-vector-machine-intro-machine-learning-tutorial/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Use the scikit-learn implementation of \n",
        "[SVM](http://scikit-learn.org/stable/modules/svm.html) with the default parameters. (You are not expected to perform any hyperparameter tuning, but feel free to do it if you think it gives you good insights for the discussion in question 5.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LnzNtQBV8gr"
      },
      "source": [
        "#### (Q3.1): Train SVM and compare to Naive Bayes (2pts)\n",
        "\n",
        "Train an SVM classifier (sklearn.svm.LinearSVC) using the features collected for Naive Bayes. Compare the\n",
        "classification performance of the SVM classifier to that of the Naive\n",
        "Bayes classifier with smoothing.\n",
        "Use cross-validation to evaluate the performance of the classifiers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 441,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_features_svm(corpus, use_pos=False, stem=False, allow_closed_classes=True):\n",
        "    '''\n",
        "    Extract features from corpus using token counts.\n",
        "    Input:\n",
        "        corpus: training corpus\n",
        "        use_pos: a boolean indicating whether to use part-of-speech tags\n",
        "        stem: a boolean indicating whether to use stemming\n",
        "        allow_closed_classes: a boolean indicating whether to allow closed classes\n",
        "    Output:\n",
        "        features: a list of preprocessed strings corresponding to each text in the corpus\n",
        "    '''\n",
        "    stemmer = PorterStemmer() if stem else None\n",
        "    features = []\n",
        "    for doc in corpus:\n",
        "        text = ''\n",
        "        for sentence in doc['content']:\n",
        "            for word, pos_tag in sentence:\n",
        "                if not allow_closed_classes and not pos_tag.startswith(('NN', 'JJ', 'VB', 'RB')):\n",
        "                    continue\n",
        "                word = word.lower()\n",
        "                if stem:\n",
        "                    if word in stem_dict:\n",
        "                        word = stem_dict[word]\n",
        "                    else:\n",
        "                        word = stemmer.stem(word)\n",
        "                        stem_dict[word] = word\n",
        "                        word = stemmer.stem(word)\n",
        "                if use_pos:\n",
        "                    text += f'{word}_{pos_tag} '\n",
        "                else:\n",
        "                    text += f'{word} '\n",
        "        features.append(text)\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 527,
      "metadata": {},
      "outputs": [],
      "source": [
        "def kfold_cv_svc(reviews, use_pos=False, stem=False, allow_closed_classes=True, n_folds=10):\n",
        "    '''\n",
        "    Implement K-fold cross-validation for the SVC with rounding-robin splitting.\n",
        "    Input:\n",
        "        reviews: a list of reviews\n",
        "        use_pos: a boolean indicating whether to use part-of-speech tags\n",
        "        stem: a boolean indicating whether to use stemming\n",
        "        allow_closed_classes: a boolean indicating whether to allow closed classes\n",
        "        n_folds: the number of folds\n",
        "    Output:\n",
        "        accuracies: a list of accuracies for each fold\n",
        "    '''\n",
        "    accuracies = []\n",
        "    count_vectorizer = CountVectorizer(lowercase=False)\n",
        "    for k in tqdm(range(n_folds)):\n",
        "        reviews_val = reviews[k::n_folds]\n",
        "        reviews_train = [r for r in reviews if r not in reviews_val]\n",
        "        X_train = extract_features_svm(reviews_train, use_pos, stem, allow_closed_classes)\n",
        "        X_train = count_vectorizer.fit_transform(X_train)\n",
        "        y_train = [r['sentiment'] for r in reviews_train]\n",
        "        X_val = extract_features_svm(reviews_val, use_pos, stem, allow_closed_classes)\n",
        "        X_val = count_vectorizer.transform(X_val)\n",
        "        y_val = [r['sentiment'] for r in reviews_val]\n",
        "        svc = LinearSVC(max_iter=10000)\n",
        "        svc.fit(X_train, y_train)\n",
        "        accuracies.append(svc.score(X_val, y_val))\n",
        "    mean_accuracy = sum(accuracies) / len(accuracies)\n",
        "    var_accuracy = sum([(a - mean_accuracy)**2 for a in accuracies]) / len(accuracies)\n",
        "    \n",
        "    return mean_accuracy, var_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 530,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:23<00:00,  2.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average accuracy of SVC (no POS and all classes) with 10-fold CV: 0.834\n",
            "Variance of accuracy of SVC (noi POS and all classes) with 10-fold CV: 0.00075\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the SVC classifier with K-fold CV\n",
        "warnings.filterwarnings('ignore')   # ignore convergence warnings\n",
        "svc_acc_cv_mean, svc_acc_cv_var = kfold_cv_svc(reviews_train, n_folds=10, use_pos=False, stem=False, allow_closed_classes=True)\n",
        "print(f'Average accuracy of SVC (no POS and all classes) with 10-fold CV: {svc_acc_cv_mean:1.3f}')\n",
        "print(f'Variance of accuracy of SVC (noi POS and all classes) with 10-fold CV: {svc_acc_cv_var:1.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The SVM classifier exhibits similar performance to the Naive Bayes classifier with smoothing (accuracy ~83% over 10-folds on training set). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifXVWcK0V9qY"
      },
      "source": [
        "### POS disambiguation (2pts)\n",
        "\n",
        "Now add in part-of-speech features. You will find the\n",
        "movie review dataset has already been POS-tagged for you ([here](https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf) you find the tagset). Try to\n",
        "replicate the results obtained by Pang et al. (2002).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA3I82o4oWGu"
      },
      "source": [
        "####(Q3.2) Replace your features with word+POS features, and report performance with the SVM. Use cross-validation to evaluate the classifier and compare the results with (Q3.1). Does part-of-speech information help? Explain why this may be the case. (1pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 543,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:26<00:00,  2.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average accuracy of SVC (with POS) with 10-fold CV: 0.840\n",
            "Variance of accuracy of SVC (with POS) with 10-fold CV: 0.00046\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the SVC classifier with POS with K-fold CV\n",
        "svc_pos_acc_cv_mean, svc_pos_acc_cv_var = kfold_cv_svc(reviews_train, n_folds=10, use_pos=True, stem=False, allow_closed_classes=True)\n",
        "print(f'Average accuracy of SVC (with POS) with 10-fold CV: {svc_pos_acc_cv_mean:1.3f}')\n",
        "print(f'Variance of accuracy of SVC (with POS) with 10-fold CV: {svc_pos_acc_cv_var:1.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0dt_oQupUNe"
      },
      "source": [
        "The accuracy remians roughly the same (around 84%) when using POS tags in the features. So POS disambiguation does not help much. It could be that the words convey the same sentiment (or no sentiment) even when used in different parts-of-speech. Or it could be simply because POS disambiguation is not very common in the dataset so it does not have a large impact on performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su-3w87eMW0w"
      },
      "source": [
        "#### (Q3.3) Discard all closed-class words from your data (keep only nouns, verbs, adjectives, and adverbs), and report performance. Does this help? Use cross-validation to evaluate the classifier and compare the results with (Q3.2). Are closed-class words detrimental to the classifier? Explain why this may be the case. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 493,
      "metadata": {
        "id": "CCUPlPozCYUX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:12<00:00,  1.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average accuracy of SVC (with POS) with 10-fold CV: 0.843\n",
            "Variance of accuracy of SVC (with POS) with 10-fold CV: 0.00056\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the SVC classifier with POS and no closed classes with K-fold CV\n",
        "svc_pos_nocc_acc_cv_mean, svc_pos_nocc_acc_cv_var = kfold_cv_svc(reviews_train, n_folds=10, use_pos=True, stem=False, allow_closed_classes=False)\n",
        "print(f'Average accuracy of SVC (with POS and no closed classes) using 10-fold CV: {svc_pos_nocc_acc_cv_mean:1.3f}')\n",
        "print(f'Variance of accuracy of SVC (with POS and no closed classes) using 10-fold CV: {svc_pos_nocc_acc_cv_var:1.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaxCVrs8pWSp"
      },
      "source": [
        "The performance does not change much from ealier (accuracy ~84\\%). Intuitively, using only open-class words makes more sense as these convey the most information about the sentiment, while closed-class words do not convey much. So we do not lose much information by discarding the closed-class works, as our results indicate. But we also don't get a large performance boost, perhaps because the SVM algorithm was already assigning small weights to those features. But it would still make sense to discard them since we can compress the feature space without losing information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfwqOciAl2No"
      },
      "source": [
        "# (Q4) Discussion (max. 500 words). (5pts)\n",
        "\n",
        "> Based on your experiments, what are the effective features and techniques in sentiment analysis? What information do different features encode?\n",
        "Why is this important? What are the limitations of these features and techniques?\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYuse5WLmekZ"
      },
      "source": [
        "#### Lexicon-based approach\n",
        "We use a lexicon containing the polarity (and magnitude) of words in the vocabulary. Then we compute the overall sentiment of an article as the (magnitude-weighted) sum of their constituent words, while accounting for positive bias (using either a absolute or relative threshold). The performance is decent (~67–70% accuracy on our entire dataset), it is limited by the fact that the lexicon is *fixed* (as it does not learn from the data) and *per-word*. It cannot adapt to the specific context in the corpus, and also cannot learn relationships between individual words and between the words and the article context. Using magnitude information and relative thresholding improve the performance somewhat, but the method is inherently too simplistic to be effective.\n",
        "\n",
        "#### Naive Bayes\n",
        "Unlike the lexicon-based approach, Naive Bayes actually *learns* the priors and conditional probabilities from the text. We see a drastic improvement on our dataset with NB, getting an accuracy of ~83% on the test set and ~82% using 10-fold cross-validation on the training set, using only unigrams. One challenge in NB is dealing with *unseen words* which occur in only one class, or only in the test set, which could lead to absurd results as the conditional probabilities are zero (and the log-likelihood undefined). One solution is to discard all such words, while another is *Laplacian smoothing*. Both approaches yield similar results in our test dataset (~83%, similar to the baseline). We can also use *stemming* to retain only the root forms of the word; this allows feature compression without much information loss (in our dataset we reduce the vocabulary size from ~45000 to ~ 32000 while the 10-fold CV accuracy stays at around 82%). Finally, we can include more ngrams in the model to capture phrases and contexts better, leading to a richer model. However, this greatly increases computational complexity (order of polynomial time) and also requires a much larger dataset (else we would get very sparse models as higher ngrams occur rarely). We see this problem in our dataset, as including bigrams and trigrams increases the vocabulary by an order of magnitude each time, but the performance stays the same (~82% CV acc. over 10 folds), probably because our dataset is too small for these ngrams.\n",
        "\n",
        "#### SVM\n",
        "SVM differs from Naive Bayes in that it is non-probabilistic and simply tries to maximize separation between points in different classes. Here the features are the token counts in the corpus. Its performance on the training dataset is similar to NB (~83% CV accuracy over 10 folds). This doesn't change on incorporating part-of-speech tags in the features, perhaps because different POS forms convey the same sentiment, or there is not much POS disambiguation in the data. We could also discard closed-class words as they do not convey sentiment, thus acheiving dimensionality reduction without much information loss. In our dataset the performance remain roughly the same, around 84% CV accuracy (10 folds). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwaKwfWQhRk_"
      },
      "source": [
        "# Submission \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOUeaET5ijk-"
      },
      "source": [
        "Abhinav Bhuyan, 12434817\n",
        "\n",
        "Job Gräber 12440272"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A9K-H6Tii3X"
      },
      "source": [
        "**That's it!**\n",
        "\n",
        "- Check if you answered all questions fully and correctly. \n",
        "- Download your completed notebook using `File -> Download .ipynb` \n",
        "- Check if your answers are all included in the file you submit.\n",
        "- Submit your .ipynb file via *Canvas*. One submission per group. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 ('nlp1')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "88cacb807bc361ab1701b7cbf9beeaf8fbb9d32c7f93f658c5559a55de88a0a4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
